<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>quekai&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.guzhipin.top/"/>
  <updated>2019-08-04T14:15:26.800Z</updated>
  <id>http://www.guzhipin.top/</id>
  
  <author>
    <name>quekai</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python数据挖掘入门与实践</title>
    <link href="http://www.guzhipin.top/2019/08/04/Python%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%85%A5%E9%97%A8%E4%B8%8E%E5%AE%9E%E8%B7%B5/"/>
    <id>http://www.guzhipin.top/2019/08/04/Python数据挖掘入门与实践/</id>
    <published>2019-08-04T14:13:11.000Z</published>
    <updated>2019-08-04T14:15:26.800Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Python数据挖掘入门与实践"><a href="#Python数据挖掘入门与实践" class="headerlink" title="Python数据挖掘入门与实践"></a>Python数据挖掘入门与实践</h1><h2 id="第一章-开始数据挖掘之旅"><a href="#第一章-开始数据挖掘之旅" class="headerlink" title="第一章 开始数据挖掘之旅"></a>第一章 开始数据挖掘之旅</h2><h3 id="1-1-数据挖掘简介"><a href="#1-1-数据挖掘简介" class="headerlink" title="1.1 数据挖掘简介"></a>1.1 数据挖掘简介</h3><ul><li><p>数据集主要包括以下两个部分：</p><p>表示真实世界中物体的样本。</p><p>描述数据集中样本的特征。</p></li><li><p>特征抽取是数据挖掘过程的一个重要环节。</p></li></ul><h3 id="1-2-使用Python和IPython-Notebook"><a href="#1-2-使用Python和IPython-Notebook" class="headerlink" title="1.2 使用Python和IPython Notebook"></a>1.2 使用Python和IPython Notebook</h3><ul><li><code>pip3 freeze</code>命令测试pip能否正常运行。</li><li>只为当前用户安装ipython 可用命令 <code>pip install --user ipython[all]</code></li><li>使用命令 <code>jupyter notebook</code>创建IPython Notebook实例，并打开web浏览器连接到实例。ctrl+C关闭。</li><li>pip安装scikit-learn</li></ul><h3 id="1-3-亲和性分析示例"><a href="#1-3-亲和性分析示例" class="headerlink" title="1.3 亲和性分析示例"></a>1.3 亲和性分析示例</h3><ul><li><p>亲和性分析：根据样本个体之间的相似度，确定它们关系的亲疏。应用场景如下</p><p>向网站用户提供多样化服务或投放定向广告</p><p>为用户推荐电影或商品，而卖给他们一些与之相关的玩意</p><p>根据基因寻找有亲缘关系的人</p></li><li><p>商品推荐。根据数据挖掘，我们希望得到以下规则：如果一个人买了商品X，那么他很有可能购买商品Y。</p></li><li><p>本书源码来自PacktPublishing</p></li><li><p>规则的优劣常见的衡量方法是支持度和置信度。</p></li><li><p>支持度指数据集中规则应验的次数，有时需要对支持度进行规范化（除以总数量）。</p></li><li><p>置信度衡量的是规则准确率如何，即符合前提条件的所有规则里，跟当前规则结论一致的比例有多大。</p></li><li><p>分别为规则应验和规则无效这两种情况创建字典。字典的键是由条件和结论组成的元组，如（3，4）.</p></li><li><p><code>from collections import defaultdict</code>使用defaultdict使得查找的键不存在时可以返回默认值</p></li><li><p><code>from operator import itemgetter</code>可以获取字典各元素的值，<code>itemgetter(1)</code></p></li></ul><h3 id="1-4-什么是分类"><a href="#1-4-什么是分类" class="headerlink" title="1.4 什么是分类"></a>1.4 什么是分类</h3><ul><li>分类应用的目标是，根据已知类别的数据集，经过训练得到一个分类模型，再用模型对类别未知的数据进行分类。</li><li>过拟合：模型在训练集上表现很好，但对于没有见过的数据表现很差。不要使用训练数据测试算法。</li><li><code>from sklearn.model_selection import train_test_split</code> 可将数据集划分为训练集和测试集。</li></ul><h2 id="第二章-用scikit-learn估计器分类"><a href="#第二章-用scikit-learn估计器分类" class="headerlink" title="第二章 用scikit-learn估计器分类"></a>第二章 用scikit-learn估计器分类</h2><h3 id="2-1-scikit-learn-估计器"><a href="#2-1-scikit-learn-估计器" class="headerlink" title="2.1 scikit-learn 估计器"></a>2.1 scikit-learn 估计器</h3><ul><li>估计器：用于分类、聚类和回归分析</li><li>转换器：用于数据预处理和数据转换</li><li>流水线：组合数据挖掘流程，便于再次使用。</li><li>sklearn估计器包括fit()和predict()两个函数，接收和输出格式均为numpy数组或类似格式。</li><li>近邻算法：查找训练集，找到和新个体最相似的哪些个体，看这些个体属于哪个类别，就把新个体分到哪个类别。要计算每两个个体之间的距离，计算量大。在特征取离散值的数据集上表现很差。</li><li>欧式距离：即真实距离，是两个特征向量长度平方和的平方根。直观，但当某些特征取值比其他特征大得多，或很多特征值为0即稀疏矩阵时，结果不准确，此时可使用曼哈顿距离和余弦距离。</li><li>曼哈顿距离为两个特征在标准坐标系中的绝对轴距之和，受异常值的影响比欧氏距离小。但当某些特征取值比其他特征大得多，这些特征会掩盖其他特征间的近邻关系。</li><li>余弦距离更适合解决异常值和数据稀疏问题，指的是特征向量夹角的余弦值。适用于特征向量很多的情况，丢弃了向量长度所包含的在某些场景下有用的信息。</li><li>交叉检验算法描述如下：<ul><li>将整个大数据集分为几个部分</li><li>对于每个部分执行以下操作：<ul><li>将其中一部分作为当前测试集</li><li>用剩余部分训练算法</li><li>在当前测试集上测试算法</li></ul></li><li>记录每次得分及平均得分</li><li>在上述过程中，每条数据只能在测试集中出现一次，以减少运气成分。</li></ul></li><li>from sklearn.model_selection import cross_val_score 默认使用Stratified K Fold方法切分数据集，保证切分后的数据集中类别分别大致相同。用此函数进行交叉检验。</li><li><code>%matplotlib inline</code> 来告知要在notebook里作图</li></ul><h3 id="2-2-流水线在预处理中的应用"><a href="#2-2-流水线在预处理中的应用" class="headerlink" title="2.2 流水线在预处理中的应用"></a>2.2 流水线在预处理中的应用</h3><ul><li>规范化：特征值的大小和该特征的分类效果没有任何关系，所以要对不同的特征进行规范化，使得它们的特征落在相同的值域或几个确定的类别。</li><li>选取最具区分度的特征、创建新特征都属于预处理的范畴。sklearn中的预处理工具叫做转换器。</li><li><code>from sklearn.preprocessing import MinMaxScaler</code>进行基于特征的规范化，把每个特征值域规范到0和1之间。<ul><li>为使每条数据特征值和为1，使用Normalizer</li><li>为使各特征的均值为0，使用StandardScaler</li><li>为将数值型特征二值化，使用Binarizer，大于阈值为1，反之为0</li></ul></li></ul><h3 id="2-3-流水线"><a href="#2-3-流水线" class="headerlink" title="2.3 流水线"></a>2.3 流水线</h3><ul><li>随着实验的增加，操作复杂程度也在提高，可能导致错误操作或操作顺序不当的问题。流水线就是用来解决这个问题的。流水线将这些步骤保存到工作流中，以便之后的数据读取以及预处理等操作。</li><li><code>from sklearn.pipeline import Pipeline</code> 流水线的输入为一连串的数据挖掘步骤，接着是转换器，最后是估计器，每一步的结果作为下一步的输出。</li><li>每一步都用元组(‘名称’,’步骤’)来表示。如<code>scaling_pipline = Pipeline([(&#39;scale&#39;, MinMaxScaler()), (&#39;predict&#39;, KNeighborsClassifier())])</code></li></ul><h2 id="第三章-用决策树预测获胜球队"><a href="#第三章-用决策树预测获胜球队" class="headerlink" title="第三章 用决策树预测获胜球队"></a>第三章 用决策树预测获胜球队</h2><h3 id="3-1-加载数据集"><a href="#3-1-加载数据集" class="headerlink" title="3.1 加载数据集"></a>3.1 加载数据集</h3><ul><li>决策树的一大优点是人和及其都能看懂</li><li>pandas.readcsv函数提供了修复数据的参数，<code>pd.read_csv(data_filename, parse_dates=[&quot;Dates&quot;], skiprows=[0,])</code></li><li>用<code>dataset.columns=[]</code>修改头部。</li><li>用<code>dataset[].values</code>提取数组。</li><li>用<code>for index, row in dataset.iterrows()</code>遍历每一行。用for index, row in dataset.sort(“Date”).iterrows()按某一列顺序遍历。</li></ul><h3 id="3-2-决策树"><a href="#3-2-决策树" class="headerlink" title="3.2 决策树"></a>3.2 决策树</h3><ul><li>决策树是一种积极算法，需要进行训练，而近邻算法是惰性算法，分类时才开始干活。</li><li>决策树在从根节点起每层选取该层的最佳特征用于决策，到达下一个节点，选择下一个最佳特征，以此类推。当无法从增加树的层级中获得更多信息时，启动退出机制。</li><li>sklearn实现了分类回归树算法（CART）并将其作为生成决策树的默认算法，支持连续型和类别型特征。</li><li>退出准则可以防止过拟合。除了退出准则外，也可以先建立完整的树，再进行剪枝，去掉对整个过程没有提供太多信息的节点。</li><li>使用<ul><li>min_samples_split：指定创建一个新节点至少需要的个体数量</li><li>min_samples_leaf：指定为保留节点每个节点至少应该保留的个体数量。</li></ul></li><li>创建决策的标准，有：<ul><li>基尼不纯度：用于衡量决策节点错误预测新个体类别的比例。</li><li>信息增益：用信息论中的熵表示决策节点提供多少新信息。</li></ul></li><li><code>from sklearn.tree import DecisionTreeClassifier</code>创建决策树。</li></ul><h3 id="3-3-NBA比赛结果预测"><a href="#3-3-NBA比赛结果预测" class="headerlink" title="3.3 NBA比赛结果预测"></a>3.3 NBA比赛结果预测</h3><ul><li><p><code>from sklearn.preprocessing import LabelEncoder</code>转换器将字符串类型的球队名转化为整型，以满足sklearn决策树的需求。</p><p>encoding = LabelEncoder()</p><p>encoding.fit(dataset[“”].values)</p><p>home_teams = encoding.transform(dataset[“”].values)</p></li><li><p>np.vstack，np.hstack将向量组合起来，形成一个矩阵。</p></li><li><p>使用LabelEncoder转换得到的整型仍被认为是连续型特征，即1与2比1与3更相似。</p></li><li><p>使用OneHotEncoder可以将整数转化为二进制数字，特征有多少种类型就有多少位二进制数字，第几个类型第几个二进制位为1，其余为0。如001，010，100.</p></li></ul><h3 id="3-4-随机森林"><a href="#3-4-随机森林" class="headerlink" title="3.4 随机森林"></a>3.4 随机森林</h3><ul><li><p>决策树可能出现过拟合的情况，解决方法之一是调整决策树算法，限制它所学到的规则的数量。使用这种方法会导致决策树泛化能力强，但整体表现稍弱。</p></li><li><p>随机森林通过创建多棵决策树，用它们分别进行预测，再根据少数服从多少的原则选择最终预测结果。</p><ul><li>装袋：每次随机从数据集中选取一部分数据作为训练集。</li><li>选取部分决策特征作为决策依据：前几个决策节点的特征非常突出，随机选取的训练集仍具有较大相似性。</li></ul></li><li><p>方差是由训练集的变化引起的。决策树这种方差大的算法极易受到训练集变化的影响，从而产生过拟合问题。随机森林对大量决策树的预测结果取均值，能有效降低方差。</p></li><li><p>偏误是由算法的假设引起的，与数据集没有关系。</p></li><li><p>决策树集成做出了以下假设：预测过程具有因分类器而异的随机性，使用多个模型得到的预测结果的均值，能够消除随机误差的影响。</p></li><li><p><code>from sklearn.ensemble import RandomForestClassifier</code>提供了<code>DecisionTreeClassifier</code>的的参数，如决策标准（基尼不纯度/信息增益）、max_features、min_samples_split等。也引入了新参数：</p><ul><li>n_esimators：指定决策树数量。</li><li>oob_score：设为真，则 测试时不会使用训练时用过的数据。</li><li>n_jobs：并行计算使用的内核数量。</li></ul></li><li><p>使用<code>GridSearchCV()</code>搜索最佳参数：</p><p><code>parameter_space = {</code></p><pre><code>`&quot;max_features&quot;: [2, 10, &apos;auto&apos;],``&quot;n_estimators&quot;: [100,],``&quot;criterion&quot;: [&quot;gini&quot;, &quot;entropy&quot;],``&quot;min_samples_leaf&quot;: [2, 4, 6],``}`</code></pre><p><code>clf = RandomForestClassifier(random_state=14)</code><br><code>grid = GridSearchCV(clf, parameter_space)</code></p><p>可使用<code>grid.best_estimator_</code>查看使用了哪些参数。</p></li><li><p>使用<code>dataset[&quot;&quot;] = feature_creator(dataset)</code>创建新特征。</p></li></ul><h2 id="第四章-用亲和性分析推荐电影"><a href="#第四章-用亲和性分析推荐电影" class="headerlink" title="第四章 用亲和性分析推荐电影"></a>第四章 用亲和性分析推荐电影</h2><h3 id="4-1-亲和性分析"><a href="#4-1-亲和性分析" class="headerlink" title="4.1 亲和性分析"></a>4.1 亲和性分析</h3><ul><li>亲和性分析用来找出两个对象共同出现的情况。应用场景如：<ul><li>欺诈检测</li><li>顾客区分</li><li>软件优化</li><li>产品推荐</li></ul></li><li>Apriori算法是经典的亲和性分析算法。从数据集中频繁出现的商品中选取共同出现的商品组成频繁项集，避免复杂度呈指数增长的问题。<ul><li>最小支持度：要生存A,B的频繁项集（A,B），要求最小支持度为30，则A和B都必须在数据集中出现30次。更大的频繁项集如（A,B,C）的子集（A,B）也要是满足最小支持度的频繁项集。</li><li>生成频繁项集后，再考虑其他不够频繁的项集。</li></ul></li><li>Apriori算法过程为：<ul><li>设定最小支持度，找出频繁项集。</li><li>根据置信度选取关联规则。</li></ul></li></ul><h3 id="4-2-电影推荐问题"><a href="#4-2-电影推荐问题" class="headerlink" title="4.2 电影推荐问题"></a>4.2 电影推荐问题</h3><ul><li><code>all_ratings = pd.read_csv(ratings_filename, delimiter=&#39;\t&#39;, header=None, names=[&quot;UserID&quot;, &quot;MovieID&quot;, &quot;Rating&quot;, &quot;Datetime&quot;])</code>将识别制表符作为分隔符，没有表头，添加表头。</li><li><code>all_ratings[&quot;Datetime&quot;] = pd.to_datetime(all_ratings[&quot;Datetime&quot;],unit=&#39;s&#39;)</code>解析时间戳数据，设定单位为秒。</li><li>稀疏矩阵格式：即对不存在的数据不存储，而不是存放大量的0。</li></ul><h3 id="4-3-Apriori算法的实现"><a href="#4-3-Apriori算法的实现" class="headerlink" title="4.3 Apriori算法的实现"></a>4.3 Apriori算法的实现</h3><ul><li><p><code>ratings = all_ratings[all_ratings[&quot;UserID&quot;].isin(range(200))]</code> 选取一部分数据作训练集，减少搜索空间。</p></li><li><p><code>favorable_ratings = ratings[ratings[&quot;Favorable&quot;]]</code> 新建数据集，只保留某一行。</p></li><li><p><code>favorable_reviews_by_users = dict((k,frozenset(v.values)) for k, v in favorable_ratings.groupby(&quot;UserID&quot;)[&quot;MovieID&quot;])</code>用.groupby进行分组，frozenset是固定不变的集合，速度快于列表。</p></li><li><p>Apriori算法过程如下：</p><ul><li>把各项放到只包含子集的项集中，生成最初的频繁项集。只使用达到最小支持度的项。</li><li>查找现有频繁项集的超集，发现新的备选项集。</li><li>测试新生成备选项集的频繁程度，如果不够频繁则舍弃。如果没有新的频繁项集，则跳到最后一步。</li><li>存储新发现的频繁项集，跳到第二步。</li><li>返回所有频繁项集。</li></ul></li><li><p>第一步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> frequent_itemsets[<span class="number">1</span>] = dict((frozenset((movie_id,)),row[<span class="string">"Favorable"</span>]) </span><br><span class="line"><span class="keyword">for</span> movie_id, row <span class="keyword">in</span> num_favorable_by_movie.iterrows() </span><br><span class="line"><span class="keyword">if</span> row[<span class="string">"Favorable"</span>] &gt; min_support)</span><br></pre></td></tr></table></figure></li><li><p>第二三步：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_frequent_itemsets</span><span class="params">(favorable_reviews_by_users, k_1_itemsets, min_support)</span>:</span></span><br><span class="line">    counts = defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> user, reviews <span class="keyword">in</span> favorable_reviews_by_users.items():</span><br><span class="line">        <span class="keyword">for</span> itemset <span class="keyword">in</span> k_1_itemsets:</span><br><span class="line">            <span class="keyword">if</span> itemset.issubset(reviews):</span><br><span class="line">                <span class="keyword">for</span> other_reviewed_movie <span class="keyword">in</span> reviews - itemset:</span><br><span class="line">                    current_superset = itemset | frozenset((other_reviewed_movie,))</span><br><span class="line">                    counts[current_superset] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> dict([(itemset, frequency)</span><br><span class="line">               <span class="keyword">for</span> itemset, frequency <span class="keyword">in</span> counts.item()</span><br><span class="line">               <span class="keyword">if</span> frequency &gt;= min_support])</span><br></pre></td></tr></table></figure></li><li><p>第四五步</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">20</span>):</span><br><span class="line">    cur_frequent_itemsets = find_frequent_itemsets(favorable_reviews_by_users, frequent_itemsets[k<span class="number">-1</span>], min_support)</span><br><span class="line">    frequent_itemsets[k] = cur_frequent_itemsets</span><br><span class="line">    <span class="keyword">if</span> len(cur_frequent_itemsets) == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"Did not find any frequent itemsets of length &#123;&#125;"</span>.format(k))</span><br><span class="line">        sys.stdout.flush()</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"I found &#123;&#125; frequent itemsets of length &#123;&#125;"</span>.format(len(cur_frequent_itemsets), k))</span><br><span class="line">        sys.stdout.flush()</span><br><span class="line"><span class="keyword">del</span> frequent_itemsets[<span class="number">1</span>]</span><br></pre></td></tr></table></figure></li></ul><h2 id="第五章-用转换器抽取特征"><a href="#第五章-用转换器抽取特征" class="headerlink" title="第五章 用转换器抽取特征"></a>第五章 用转换器抽取特征</h2><h3 id="5-1-特征抽取"><a href="#5-1-特征抽取" class="headerlink" title="5.1 特征抽取"></a>5.1 特征抽取</h3><ul><li>特征抽取是数据挖掘任务最重要的环境，对最终结果的影响高于数据挖掘算法本身。</li><li>特征选择降低真实世界的复杂度，模型比现实更容易操纵。</li><li>简化要以数据挖掘的目标为核心。</li><li>简化会忽略很多细节，甚至会抛弃很多对数据挖掘算法能力起到帮助作用的信息。</li><li>不是所有特征必须是数值型或类别型值，直接作用于文本、图像和其他数据结构的算法已经研究出来了。</li><li><code>adult.dropna(how=&quot;all&quot;, inplace=True)</code>删除包含无效数据的行，inplace为真表明在当前数据框中修改，而不是新建一个数据框。</li><li><code>adult[&quot;Hours-per-week&quot;].describe()</code>提供了常见统计量的计算。</li><li><code>adult[&quot;Work-Class&quot;].unique()</code>得到特征的所有不同情况。</li></ul><h3 id="5-2-特征选择"><a href="#5-2-特征选择" class="headerlink" title="5.2 特征选择"></a>5.2 特征选择</h3><ul><li><p>特征选择的原因如下：</p><ul><li>降低复杂度</li><li>降低噪音</li><li>增加模型可读性</li></ul></li><li><p><code>X = np.arange(30).reshape((10, 3))</code>创建0到29，30个数字，分为3列10行。</p></li><li><p>删除方差达不到最低标准的特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> VarianceThreshold</span><br><span class="line">vt = VarianceThreshold()</span><br><span class="line">xt = vt.fit_transform(X)</span><br><span class="line">print(vt.variances_)</span><br></pre></td></tr></table></figure></li><li><p>随着特征的增加，寻找最佳特征组合的时间复杂度是呈指数级增长的，变通的方法是寻找表现好的单个特征，一般是测量变量与目标类别之间的某种相关性。</p></li><li><p>SelectKBest返回k个最佳特征，SelectPercentile返回最佳的前r%个特征。计算单个特征与某一类别之间相关性的计算方法有卡方检验，互信息和信息熵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line">transformer = SelectKBest(score_func=chi2, k=<span class="number">3</span>)</span><br><span class="line">xt_chi2 = transformer.fit_transform(X, y)</span><br><span class="line">print(transformer.scores_)</span><br></pre></td></tr></table></figure></li><li><p>也可使用皮尔逊相关系数计算相关性。皮尔逊相关系数为-1到1的值，绝对值越大，相关性越大。<code>from scipy.stats import pearsonr</code>scipy实现的皮尔逊相关系数接收（X，y），返回每个特征的皮尔逊相关系数和p值，X为该特征列。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multivariate_pearsonr</span><span class="params">(X, y)</span>:</span></span><br><span class="line">    scores, pvalues = [], []</span><br><span class="line">    <span class="keyword">for</span> column <span class="keyword">in</span> range(X.shape[<span class="number">1</span>]):</span><br><span class="line">        cur_score, cur_p = pearsonr(X[:, column], y)</span><br><span class="line">        scores.append(abs(cur_score))</span><br><span class="line">        pvalues.append(cur_p)</span><br><span class="line">    <span class="keyword">return</span> (np.array(scores), np.array(pvalues))</span><br><span class="line">transformer = SelectKBest(score_func=multivariate_pearsonr, k=<span class="number">3</span>)</span><br><span class="line">xt_pearson = transformer.fit_transform(X, y)</span><br><span class="line">print(transformer.scores_)</span><br></pre></td></tr></table></figure></li><li><p>哪些特征是好的没有标准答案，取决于度量标准。</p></li></ul><h3 id="5-3-创建特征"><a href="#5-3-创建特征" class="headerlink" title="5.3 创建特征"></a>5.3 创建特征</h3><ul><li><p>数据集中原始特征可能会出现特征间相关性很强，特征冗余等情况，增加算法除了难度，因此要创建新特征。</p></li><li><p>使用converters修复数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_number</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">return</span> float(x)</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        <span class="keyword">return</span> np.nan</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">converters = defaultdict(convert_number)</span><br><span class="line">converters[<span class="number">1558</span>] = <span class="keyword">lambda</span> x: <span class="number">1</span> <span class="keyword">if</span> x.strip() == <span class="string">'ad.'</span> <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">ads = pd.read_csv(data_filename, header=<span class="literal">None</span>, converters=converters)</span><br></pre></td></tr></table></figure></li><li><p>主成分分析算法的目的是找到能用较少信息描述数据集的特征组合。意在发现彼此之间没有相关性、能够描述数据集、特征方差与整体方差相近的特征，即主成分。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">5</span>)</span><br><span class="line">Xd = pca.fit_transform(X)</span><br><span class="line">np.set_printoptions(precision=<span class="number">3</span>, suppress=<span class="literal">True</span>)</span><br><span class="line">pca.explained_variance_ratio_</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>用PCA算法不好的地方在于，主成分往往是很多特征的复杂组合，理解起来很困难。</p></li><li><p>PCA的一个优点是可以将抽象的数据集绘制成图像，如将前两个特征做成图形。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">classes = set(y)</span><br><span class="line">colors = [<span class="string">'red'</span>, <span class="string">'green'</span>]</span><br><span class="line"><span class="keyword">for</span> cur_class, color <span class="keyword">in</span> zip(classes, colors):</span><br><span class="line">    mask = (y == cur_class).values</span><br><span class="line">    plt.scatter(Xd[mask,<span class="number">0</span>], Xd[mask,<span class="number">1</span>], marker=<span class="string">'o'</span>, color=color, label=int(cur_class))</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li></ul><h3 id="5-4-创建自己的转换器"><a href="#5-4-创建自己的转换器" class="headerlink" title="5.4 创建自己的转换器"></a>5.4 创建自己的转换器</h3><ul><li><p>导入TransformerMinxin类，重写其中的fit、transform函数。使用as_float_array判断输入类型是否为float。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> TransformerMixin</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> as_float_array</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MeanDiscrete</span><span class="params">(TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        X = as_float_array(X)</span><br><span class="line">        self.mean = X.mean(axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        X = as_float_array(X)</span><br><span class="line">        <span class="keyword">assert</span> X.shape[<span class="number">1</span>] == self.mean.shape[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> X &gt; self.mean</span><br><span class="line">mean_discrete = MeanDiscrete()</span><br><span class="line">X_mean = mean_discrete.fit_transform(X)</span><br></pre></td></tr></table></figure></li></ul><h2 id="第六章-使用朴素贝叶斯进行社会媒体挖掘"><a href="#第六章-使用朴素贝叶斯进行社会媒体挖掘" class="headerlink" title="第六章 使用朴素贝叶斯进行社会媒体挖掘"></a>第六章 使用朴素贝叶斯进行社会媒体挖掘</h2><h3 id="6-1-消歧"><a href="#6-1-消歧" class="headerlink" title="6.1 消歧"></a>6.1 消歧</h3><ul><li>朴素贝叶斯：朴素是因为假设了各特征之间是相互独立的。</li><li>文本挖掘的一个难点在于歧义，如bank一词指的是河岸还是银行，消除歧义被称为消歧。</li><li>jupyter中用<code>%%javascript</code>表示该代码段为JavaScript语言。</li><li>只有在相同的测试集上，在相同的条件下进行测试，才能比较算法的优劣。</li></ul><h3 id="6-2-文本转换器"><a href="#6-2-文本转换器" class="headerlink" title="6.2 文本转换器"></a>6.2 文本转换器</h3><ul><li>一种简单但高效的文本测量方法是统计数据集中每个单词出现的次数。</li><li><code>from collections import Counter</code>能计算列表中各个元素出现的次数，用<code>c.most_common(5)</code>输出出现次数最多的5个词。</li><li>词袋模型分为三种：<ul><li>用词语实际出现的次数作为词频。缺点是当文档长度差异明显时，词频差距会很大。</li><li>使用归一化后的词频，每篇文档中词频和为1，规避了文档长度对词频的影响。</li><li>用二值特征表示，出现为1，不出现为0。</li><li>词频-逆文档频率法（tf-idf）：用词频代替词的出现次数，词频除以包含该词的文档数。</li></ul></li><li>N元语法是指由几个连续的词组成的子序列。会导致特征矩阵变得更稀疏。另一种是字符N元语法，用于发现拼写错误。</li></ul><h3 id="6-3-朴素贝叶斯"><a href="#6-3-朴素贝叶斯" class="headerlink" title="6.3 朴素贝叶斯"></a>6.3 朴素贝叶斯</h3><ul><li><p>在贝叶斯统计学中，使用数据来描述模型，而不是用模型描述数据。频率论者则使用数据证实假设的模型。</p></li><li><p>贝叶斯定理公式如下：<br>$$<br>P(A|B)=\frac{P(B|A)P(A)}{P(B)}<br>$$<br>比较后验概率大小时，只需计算分子并比较大小。</p></li></ul><h3 id="6-4-应用"><a href="#6-4-应用" class="headerlink" title="6.4 应用"></a>6.4 应用</h3><ul><li><p>NLTK的word_tokenize函数将原始文档转换为由单词和其是否出现的字典。NLTK与转换器接口不一致，因此要创建包含fit和transform的转换器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLTKBOW</span><span class="params">(TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [&#123;word: <span class="literal">True</span> <span class="keyword">for</span> word <span class="keyword">in</span> work_tokenize(document) <span class="keyword">for</span> document <span class="keyword">in</span> X&#125;]</span><br></pre></td></tr></table></figure></li><li><p><code>from sklearn.feature_extraction import DictVectorizer</code>接收元素为字典的列表，将其转换为矩阵。</p></li><li><p><code>from sklearn.naive_bayes import BernoulliNB</code>引入二值分类的朴素贝叶斯分类器。</p></li><li><p>组合部件，创建流水线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">pipeline = Pipeline([(<span class="string">'bag-of-words'</span>, NLTKBOW()),</span><br><span class="line">                    (<span class="string">'vectorizer'</span>, DictVectorizer()),</span><br><span class="line">                    (<span class="string">'naive-bayes'</span>, BernoulliNB())</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure></li><li><p>正确率对于不均匀的数据集来说，并不能反映算法的优劣。更常用的指标为F1值。</p></li><li><p>F1值是以每个类别为基础进行定义的。包括两大概念：</p><ul><li>准确率：预测结果属于某一类的个体，实际属于该类的比例。</li><li>召回率：被正确预测为某类的个体数量与数据集中该类个体总数的比例。</li></ul></li><li><p>在案例中就是：</p><ul><li>正确率：在所有被预测为相关的消息中真正相关的占比多少？</li><li>召回率：数据集所有相关的消息中，由多少被正确识别为相关？</li></ul></li><li><p>F1值是正确率和召回率的调和平均数。<br>$$<br>F1=2·\frac{precision·recall}{precision+recall}<br>$$</p></li><li><p><code>scores=cross_val_score(pipeline, tweets, labels, scoring=&#39;f1&#39;)</code>交叉验证法计算F1得分。</p></li><li><p><code>nb = model.named_steps(&#39;naive-bayes&#39;)</code>访问流水线的每个步骤。</p></li><li><p>当概率较小时，可以使用对数概率，防止下溢。</p></li><li><p><code>np.argsort()</code>进行降序排列。</p></li><li><p>DictVectorizer保存了特征的名称，可搜索其feature_names_属性查找。</p></li></ul><h2 id="第七章-用图挖掘找到感兴趣的人"><a href="#第七章-用图挖掘找到感兴趣的人" class="headerlink" title="第七章 用图挖掘找到感兴趣的人"></a>第七章 用图挖掘找到感兴趣的人</h2><h3 id="7-1-加载数据集"><a href="#7-1-加载数据集" class="headerlink" title="7.1 加载数据集"></a>7.1 加载数据集</h3><ul><li><p>初始化twitter连接实例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> twitter</span><br><span class="line">consumer_key = <span class="string">"52Nu7ubm2szT1JyJEOB7V2lGM"</span></span><br><span class="line">consumer_secret = <span class="string">"mqA94defqjioyWeMxdJsSduthxdMMGd2vfOUKvOFpm0n7JTqfY"</span></span><br><span class="line">access_token = <span class="string">"16065520-USf3DBbQAh6ZA8CnSAi6NAUlkorXdppRXpC4cQCKk"</span></span><br><span class="line">access_token_secret = <span class="string">"DowMQeXqh5ZsGvZGrmUmkI0iCmI34ShFzKF3iOdiilpX5"</span></span><br><span class="line">authorization = twitter.OAuth(access_token, access_token_secret, consumer_key, consumer_secret)</span><br><span class="line">t = twitter.Twitter(auth=authorization, retry=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li><li><p><code>search_results = t.search.tweets(q=&quot;python&quot;, count=100)[&#39;statuses&#39;]</code>搜索包含关键词的推文。</p></li><li><p>导入joblib库，保存模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line">model_filename = os.path.join(os.path.expanduser(<span class="string">"~"</span>), <span class="string">"Models"</span>, <span class="string">"twitter"</span>, <span class="string">"python_context.pkl"</span>)</span><br><span class="line">joblib.dump(model, output_filename)</span><br></pre></td></tr></table></figure></li><li><p>定制的类无法直接用joblib加载，因此要重建NLTKBOW。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> TransformerMixin</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> word_tokenize</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NLTKBOW</span><span class="params">(TransformerMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [&#123;word: <span class="literal">True</span> <span class="keyword">for</span> word <span class="keyword">in</span> word_tokenize(document)&#125;</span><br><span class="line">                 <span class="keyword">for</span> document <span class="keyword">in</span> X]</span><br></pre></td></tr></table></figure></li><li><p>调用joblib的load函数加载模型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">context_classifier = joblib.load(model_filename)</span><br></pre></td></tr></table></figure></li><li><p><code>results = t.friends.ids(user_id=user_id, cursor=cursor, count=5000)</code>从推特获取用户关注的好友编号列表，一页5000，使用游标表示第几页，初始设为-1，不为0表示有下一页。返回一个字典，包含’ids’和下一个游标’next_cursor’。</p></li><li><p><code>friends.extend([friends for friends in results[&#39;ids&#39;]])</code>列表扩展，接收参数为一个列表。</p></li><li><p><code>sys.stdout.flush()</code>输出缓存到屏幕，避免过长等待。</p></li><li><p>完整的获取好友函数代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_friends</span><span class="params">(t, user_id)</span>:</span></span><br><span class="line">    friends = []</span><br><span class="line">    cursor = <span class="number">-1</span>  <span class="comment"># Start with the first page</span></span><br><span class="line">    <span class="keyword">while</span> cursor != <span class="number">0</span>:  <span class="comment"># If zero, that is the end:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            results = t.friends.ids(user_id=user_id, cursor=cursor, count=<span class="number">5000</span>)</span><br><span class="line">            friends.extend([friends <span class="keyword">for</span> friends <span class="keyword">in</span> results[<span class="string">'ids'</span>]])</span><br><span class="line">            cursor = results[<span class="string">'next_cursor'</span>]</span><br><span class="line">            <span class="keyword">if</span> len(friends) &gt;= <span class="number">10000</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> cursor != <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Collected &#123;&#125; friends so far, but there are more"</span>.format(len(friends)))</span><br><span class="line">                sys.stdout.flush</span><br><span class="line">        <span class="keyword">except</span> TypeError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">if</span> results <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">                print(<span class="string">"You probably reached your API limit, waiting for 5 minutes"</span>)</span><br><span class="line">                sys.stdout.flush()</span><br><span class="line">                time.sleep(<span class="number">5</span>*<span class="number">60</span>) <span class="comment"># 5 minute wait</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> e</span><br><span class="line">        <span class="keyword">except</span> twitter.TwitterHTTPError <span class="keyword">as</span> e:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">finally</span>:</span><br><span class="line">            time.sleep(<span class="number">60</span>)  <span class="comment"># Wait 1 minute before continuing</span></span><br><span class="line">    <span class="keyword">return</span> friends</span><br></pre></td></tr></table></figure></li><li><p>为加快网络构建，从现有用户-好友列表字典中计算每个好友的出现次数，降序排列，依次查看该好友是否已被查找，找到排名最高的未被查找的好友，进行搜索，更新字典，以此类推。</p></li><li><p>使用json保存/加载好友字典：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">friends_filename = os.path.join(data_folder, <span class="string">"python_friends.json"</span>)</span><br><span class="line"><span class="keyword">with</span> open(friends_filename, <span class="string">'w'</span>) <span class="keyword">as</span> outf:</span><br><span class="line">    json.dump(friends, outf)</span><br><span class="line"><span class="keyword">with</span> open(friends_filename) <span class="keyword">as</span> inf:</span><br><span class="line">    friends = json.load(inf)</span><br></pre></td></tr></table></figure></li><li><p>可以用Networkx库实现图关系的可视化。</p><ul><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 引入Networkx库，创建有向图</span></span><br><span class="line"><span class="keyword">import</span> networkx <span class="keyword">as</span> nx</span><br><span class="line">G = nx.DiGraph()</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加顶点</span></span><br><span class="line">main_users = friends.keys()</span><br><span class="line">G.add_nodes_from(main_users)</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加边</span></span><br><span class="line"><span class="keyword">for</span> user_id <span class="keyword">in</span> friends:</span><br><span class="line">    <span class="keyword">for</span> friend <span class="keyword">in</span> friends[user_id]:</span><br><span class="line">        <span class="keyword">if</span> friend <span class="keyword">in</span> main_users:</span><br><span class="line">           G.add_edge(user_id, friend)</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">nx.draw(G)</span><br></pre></td></tr></table></figure></li><li><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用plt放大图像</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.figure(<span class="number">3</span>,figsize=(<span class="number">40</span>,<span class="number">40</span>))</span><br><span class="line">nx.draw(G, alpha=<span class="number">0.1</span>, edge_color=<span class="string">'b'</span>, node_color=<span class="string">'g'</span>, node_size=<span class="number">2000</span>)</span><br><span class="line">plt.axis(<span class="string">'on'</span>)</span><br><span class="line">plt.xlim(<span class="number">0.45</span>, <span class="number">0.55</span>)</span><br><span class="line">plt.ylim(<span class="number">0.45</span>, <span class="number">0.55</span>)</span><br></pre></td></tr></table></figure></li></ul></li><li><p>杰卡德相似系数：两个集合交集的元素数量除以两个集合并集的元素数量，范围为0到1，代表两者的重合比例。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_similarity</span><span class="params">(friends1, friends2)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(friends1 &amp; friends2) / len(friends1 | friends2)</span><br></pre></td></tr></table></figure></li><li><p>创建带杰卡德相似系数权重无向图：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_graph</span><span class="params">(followers, threshold=<span class="number">0</span>)</span>:</span></span><br><span class="line">    G = nx.Graph()</span><br><span class="line">    <span class="keyword">for</span> user1 <span class="keyword">in</span> friends.keys():</span><br><span class="line">        <span class="keyword">for</span> user2 <span class="keyword">in</span> friends.keys():</span><br><span class="line">            <span class="keyword">if</span> user1 == user2:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            weight = compute_similarity(friends[user1], friends[user2])</span><br><span class="line">            <span class="keyword">if</span> weight &gt;= threshold:</span><br><span class="line">                G.add_node(user1)</span><br><span class="line">                G.add_node(user2)</span><br><span class="line">                G.add_edge(user1, user2, weight=weight)</span><br><span class="line">    <span class="keyword">return</span> G</span><br></pre></td></tr></table></figure></li><li><p>networkx中布局方式决定顶点和边的位置，常用布局方式有spring_layout，circular_layout，random_layout，shell_layout和spectral_layout。</p></li><li><p>根据布局方式，依次绘制顶点和边，获取权重数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">10</span>,<span class="number">10</span>))</span><br><span class="line">pos = nx.spring_layout(G)</span><br><span class="line">nx.draw_networkx_nodes(G, pos)</span><br><span class="line">edgewidth = [ d[<span class="string">'weight'</span>] <span class="keyword">for</span> (u,v,d) <span class="keyword">in</span> G.edges(data=<span class="literal">True</span>)]</span><br><span class="line">nx.draw_networkx_edges(G, pos, width=edgewidth)</span><br></pre></td></tr></table></figure></li></ul><h3 id="7-2-寻找子图"><a href="#7-2-寻找子图" class="headerlink" title="7.2 寻找子图"></a>7.2 寻找子图</h3><ul><li><p>聚类分析：找出相似用户群，向他们定向投放广告。</p></li><li><p>聚类分析的复杂之处在于：</p><ul><li>缺乏评价结果的标准</li><li>没有事先标注的数据进行训练，得到的是近似的分组结果，而不是明确的分类。</li></ul></li><li><p>连通分支是图中由边连接在一起的一组顶点，不要求顶点两两相连，但任意两个顶点之间存在一条路径。连通分支的计算不考虑权重，只考虑边是否存在。</p></li><li><p>用networkx的函数寻找连通分支。<code>sub_graphs = nx.connected_component_subgraphs(G)</code></p></li><li><p>画出连通分支图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sub_graphs = nx.connected_component_subgraphs(G)</span><br><span class="line">nx.draw(list(sub_graphs)[<span class="number">6</span>])</span><br></pre></td></tr></table></figure></li><li><p><code>n_subgraphs = nx.number_connected_components(G)</code>计算连通分支数量。</p></li><li><p>画出所有连通分支。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sub_graphs = nx.connected_component_subgraphs(G)</span><br><span class="line">n_subgraphs = nx.number_connected_components(G)</span><br><span class="line">fig = plt.figure(figsize=(<span class="number">20</span>, (n_subgraphs * <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">for</span> i, sub_graph <span class="keyword">in</span> enumerate(sub_graphs):</span><br><span class="line">    ax = fig.add_subplot(int(n_subgraphs / <span class="number">2</span>), <span class="number">2</span>, i)</span><br><span class="line">    ax.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    ax.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    pos = nx.spring_layout(G)</span><br><span class="line">    nx.draw_networkx_nodes(G, pos, sub_graph.nodes(), ax=ax, node_size=<span class="number">500</span>)</span><br><span class="line">    nx.draw_networkx_edges(G, pos, sub_graph.edges(), ax=ax)</span><br></pre></td></tr></table></figure></li><li><p>聚类应使得：</p><ul><li>同一簇内的个体尽可能相似</li><li>不同簇内的个体尽可能不相似</li></ul></li><li><p>轮廓系数：<br>$$<br>s=\frac{b-a}{max(a,b)}<br>$$<br>a为簇内距离，表示与簇内其他个体之间的平均距离。b为簇间距离，也就是与最近簇内个体之间的平均距离。</p></li><li><p>总轮廓系数是每个个体轮廓系数的均值。接近1时，表示簇内相似度高，簇间很远；接近0时，表示所有簇重合在一起，簇间距离很小；接近-1时，表示个体分在错误的簇内。</p></li><li><p><code>from sklearn.metrics import silhouette_score</code>计算轮廓系数。</p></li><li><p>轮廓函数的定义要求至少有两个顶点，两个连通分支。</p></li><li><p>轮廓系数函数接收距离矩阵，因此要将图转换为距离矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = nx.to_scipy_sparse_matrix(G).todense()</span><br></pre></td></tr></table></figure></li><li><p>对于稀疏矩阵，应使用V-MEASURE或调整互信息进行评价。</p></li><li><p><code>silhouette_score(X, labels, metric=&#39;precomputed&#39;)</code>指定metric，避免X被认为是特征矩阵，而不是距离矩阵。</p></li><li><p>使用<code>from scipy.optimize import minimize</code>自动调整参数优化。要求变量在其他参数前面，即：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_silhouette</span><span class="params">(threshold, friends)</span>:</span></span><br><span class="line">    G = create_graph(friends, threshold=threshold)</span><br><span class="line">    <span class="keyword">if</span> len(G.nodes()) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-99</span>  <span class="comment"># Invalid graph</span></span><br><span class="line">    sub_graphs = nx.connected_component_subgraphs(G)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> (<span class="number">2</span> &lt;= nx.number_connected_components(G) &lt; len(G.nodes()) - <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="number">-99</span>  <span class="comment"># Invalid number of components, Silhouette not defined</span></span><br><span class="line">    label_dict = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i, sub_graph <span class="keyword">in</span> enumerate(sub_graphs):</span><br><span class="line">        <span class="keyword">for</span> node <span class="keyword">in</span> sub_graph.nodes():</span><br><span class="line">            label_dict[node] = i</span><br><span class="line">    labels = np.array([label_dict[node] <span class="keyword">for</span> node <span class="keyword">in</span> G.nodes()])</span><br><span class="line">    X = nx.to_scipy_sparse_matrix(G).todense()</span><br><span class="line">    X = <span class="number">1</span> - X</span><br><span class="line">    <span class="keyword">return</span> silhouette_score(X, labels, metric=<span class="string">'precomputed'</span>)</span><br></pre></td></tr></table></figure></li><li><p>minimize是调整参数使得函数返回值最小，这里要求轮廓系数最大，因此要取反，将其变为损失函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">invert</span><span class="params">(func)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inverted_function</span><span class="params">(*args, **kwds)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> -func(*args, **kwds)</span><br><span class="line">    <span class="keyword">return</span> inverted_function</span><br></pre></td></tr></table></figure></li><li><p>设定初始阈值为0.1，设定优化方法为下山单纯形法，设定被参数参数为friends字典，设定最大迭代次数为10.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = minimize(invert(compute_silhouette), <span class="number">0.1</span>, method=<span class="string">'nelder-mead'</span>, args=(friends,), options=&#123;<span class="string">'maxiter'</span>:<span class="number">10</span>, &#125;)</span><br></pre></td></tr></table></figure><p>返回结果中的x的最佳阈值大小。</p></li></ul><h2 id="第八章-用神经网络破解验证码"><a href="#第八章-用神经网络破解验证码" class="headerlink" title="第八章 用神经网络破解验证码"></a>第八章 用神经网络破解验证码</h2><h3 id="8-1-人工神经网络"><a href="#8-1-人工神经网络" class="headerlink" title="8.1 人工神经网络"></a>8.1 人工神经网络</h3><ul><li><p>神经网络由一系列相互连接的神经元组成，每个神经元都是一个简单的函数，接收一定输入，给出相应输出。</p></li><li><p>神经元中用于处理数据的标准函数被称为激活函数。</p></li><li><p>激活函数应是可导和光滑的。常用的激活函数，如逻辑斯蒂函数：<br>$$<br>f(x)=\frac{L}{1+e^{-k(x-x_0)}}<br>$$</p></li><li><p>全连接层：上一层每个神经元的输出都输入到下一层的所有神经元。</p></li><li><p>边的权重开始时通常是随机选取的，训练过程中再逐步更新。</p></li></ul><h3 id="8-2-创建数据集"><a href="#8-2-创建数据集" class="headerlink" title="8.2 创建数据集"></a>8.2 创建数据集</h3><ul><li><p>用PIL库的Image初始化图像对象，ImageDraw初始化绘图对象，ImageFont初始化字体对象。用skimage的transform进行图像错切变化。返回归一化结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_captcha</span><span class="params">(text, shear=<span class="number">0</span>, size=<span class="params">(<span class="number">100</span>,<span class="number">24</span>)</span>)</span>:</span></span><br><span class="line">    im = Image.new(<span class="string">"L"</span>, size, <span class="string">"black"</span>)</span><br><span class="line">    draw = ImageDraw.Draw(im)</span><br><span class="line">    font = ImageFont.truetype(<span class="string">r"Coval.otf"</span>, <span class="number">22</span>)</span><br><span class="line">    draw.text((<span class="number">2</span>, <span class="number">2</span>), text, fill=<span class="number">1</span>, font=font)</span><br><span class="line">    image = np.array(im)</span><br><span class="line">    affine_tf = tf.AffineTransform(shear=shear)</span><br><span class="line">    image = tf.warp(image, affine_tf)</span><br><span class="line">    <span class="keyword">return</span> image / image.max()</span><br></pre></td></tr></table></figure></li><li><p>skimage中的label函数能找出图像中像素值相同且连接在一起的像素块。输入输出均为图像数组，返回的数组中，连接在一起的区域是大于0的值，每个区域的值不同，其他区域为0值。</p></li><li><p>skimage的regionprops能抽取连续区域，属性.bbox返回区域的起始结束横纵坐标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">segment_image</span><span class="params">(image)</span>:</span></span><br><span class="line">    labeled_image = label(image &gt; <span class="number">0</span>)</span><br><span class="line">    subimages = []</span><br><span class="line">    <span class="keyword">for</span> region <span class="keyword">in</span> regionprops(labeled_image):</span><br><span class="line">        start_x, start_y, end_x, end_y = region.bbox</span><br><span class="line">        subimages.append(image[start_x:end_x, start_y:end_y])</span><br><span class="line">    <span class="keyword">if</span> len(subimages) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> [image,]</span><br><span class="line">    <span class="keyword">return</span> subimages</span><br></pre></td></tr></table></figure></li><li><p>利用subplots返回的坐标起点，画图。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">f, axes = plt.subplots(<span class="number">1</span>, len(subimages), figsize=(<span class="number">10</span>, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(subimages)):</span><br><span class="line">    axes[i].imshow(subimages[i], cmap=<span class="string">"gray"</span>)</span><br></pre></td></tr></table></figure></li><li><p>使用<code>from sklearn.utils import check_random_state</code>随机选取字母和错切值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_sample</span><span class="params">(random_state=None)</span>:</span></span><br><span class="line">    random_state = check_random_state(random_state)</span><br><span class="line">    letter = random_state.choice(letters)</span><br><span class="line">    shear = random_state.choice(shear_values)</span><br><span class="line">    <span class="keyword">return</span> create_captcha(letter, shear=shear, size=(<span class="number">20</span>, <span class="number">20</span>)), letters.index(letter)</span><br></pre></td></tr></table></figure></li><li><p>用zip函数将3000次采样数据组合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset, targets = zip(*(generate_sample(random_state) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3000</span>)))</span><br></pre></td></tr></table></figure></li><li><p>使用一位有效码编码使得单个神经元有26个输出，为1则是该字母，否则为0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">onehot = OneHotEncoder()</span><br><span class="line">y = onehot.fit_transform(targets.reshape(targets.shape[<span class="number">0</span>],<span class="number">1</span>))</span><br></pre></td></tr></table></figure></li><li><p>用todense将稀疏矩阵转换为密集矩阵。</p></li><li><p>使用<code>from skimage.transform import resize</code>改变图像大小，将不规整的图像调到相同大小。</p></li><li><p><code>X = dataset.reshape((dataset.shape[0], dataset.shape[1] *dataset.shape[2]))</code>将数组扁平化为二维。</p></li></ul><h3 id="8-3-训练和分类"><a href="#8-3-训练和分类" class="headerlink" title="8.3 训练和分类"></a>8.3 训练和分类</h3><ul><li><p>使用pybrain进行神经网络的构建，pybrain有自己的数据格式，转换数据格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pybrain.datasets <span class="keyword">import</span> SupervisedDataSet</span><br><span class="line">training = SupervisedDataSet(X.shape[<span class="number">1</span>], y.shape[<span class="number">1</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(X_train.shape[<span class="number">0</span>]):</span><br><span class="line">    training.addSample(X_train[i], y_train[i])</span><br></pre></td></tr></table></figure></li><li><p>隐含层的神经元过多会导致过拟合，过少会导致低拟合。</p></li><li><p>导入buildNetwork函数，指定维度，创建神经网络。第一二三个参数分别为三层网络神经元的数量。激活偏执神经元。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pybrain.tools.shortcuts <span class="keyword">import</span> buildNetwork</span><br><span class="line">net = buildNetwork(X.shape[<span class="number">1</span>], <span class="number">100</span>, y.shape[<span class="number">1</span>], bias=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li><li><p>反向传播算法从输出层开始，向上层查找预测错误的神经元，微调这些神经元输入值的权重，以达到修复输出错误的目的。</p></li><li><p>微调的幅度取决于神经元各边权重的偏导数和学习速率。计算出函数误差的梯度乘以学习速率，就是原权重需要下调的幅度。</p></li><li><p>有些情况下，修正的结果仅是局部最优。</p></li><li><p>反向传播算法，限定反向传播次数为20：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pybrain.supervised <span class="keyword">import</span> BackpropTrainer</span><br><span class="line">trainer = BackpropTrainer(net, training, learningrate=<span class="number">0.01</span>, weightdecay=<span class="number">0.01</span>)</span><br><span class="line">trainer.trainEpochs(epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure></li><li><p>在trainer上调用testOnClassData函数，预测分类结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions = trainer.testOnClassData(dataset=testing)</span><br></pre></td></tr></table></figure></li><li><p>输入验证码图片，预测验证码。用activate函数激活神经网络，输入化为一维的子图片数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_captcha</span><span class="params">(captcha_image, neural_network)</span>:</span></span><br><span class="line">    subimages = segment_image(captcha_image)</span><br><span class="line">    predicted_word = <span class="string">""</span></span><br><span class="line">    <span class="keyword">for</span> subimage <span class="keyword">in</span> subimages:</span><br><span class="line">        subimage = resize(subimage, (<span class="number">20</span>, <span class="number">20</span>))</span><br><span class="line">        outputs = net.activate(subimage.flatten())</span><br><span class="line">        prediction = np.argmax(outputs)</span><br><span class="line">        predicted_word += letters[prediction]</span><br><span class="line">    <span class="keyword">return</span> predicted_word</span><br></pre></td></tr></table></figure></li><li><p>从nltk预料库中下载words语料库，从中找出长度为4的单词，并大写化。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">'words'</span>)</span><br><span class="line">valid_words = [word.upper() <span class="keyword">for</span> word <span class="keyword">in</span> words.words() <span class="keyword">if</span> len(word) == <span class="number">4</span>]</span><br></pre></td></tr></table></figure></li><li><p>用二维混淆矩阵表现预测的正确率和召回率。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm = confusion_matrix(np.argmax(y_test, axis=<span class="number">1</span>), predictions)</span><br></pre></td></tr></table></figure></li><li><p>画出混淆矩阵，并标出坐标。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">20</span>,<span class="number">20</span>))</span><br><span class="line">tick_marks = np.arange(len(letters))</span><br><span class="line">plt.xticks(tick_marks, letters)</span><br><span class="line">plt.yticks(tick_marks, letters)</span><br><span class="line">plt.ylabel(<span class="string">'Actual'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Predicted'</span>)</span><br><span class="line">plt.imshow(cm, cmap=<span class="string">"Blues"</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="8-4-用词典提升正确率"><a href="#8-4-用词典提升正确率" class="headerlink" title="8.4 用词典提升正确率"></a>8.4 用词典提升正确率</h3><ul><li><p>先检查以下词典内是否包含该单词，包含则直接输出，否则查找相似的单词，作为更新过的预测结果返回。</p></li><li><p>列文斯坦编辑距离适用于确定两个短字符串的相似度，计算一个单词变成另一个单词的步骤数，步骤数越少越相似。以下操作算一步：</p><ul><li>在单词的任意位置插入一个字母</li><li>从单词中删除任意一个字母</li><li>把一个字母替换为另一个字母</li></ul></li><li><p>nltk中实现了编辑距离算法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.metrics <span class="keyword">import</span> edit_distance</span><br><span class="line">steps = edit_distance(<span class="string">"STEP"</span>, <span class="string">"STOP"</span>)</span><br><span class="line">print(<span class="string">"The number of steps needed is: &#123;0&#125;"</span>.format(steps))</span><br></pre></td></tr></table></figure></li><li><p>在字符串等长的情况下，另一种方法是直接计算相同位置不相同的字符数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distance</span><span class="params">(prediction, word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> len(prediction) - sum(prediction[i] == word[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(prediction)))</span><br></pre></td></tr></table></figure></li><li><p>改进后的预测函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> operator <span class="keyword">import</span> itemgetter</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">improved_prediction</span><span class="params">(word, net, dictionary, shear=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">    captcha = create_captcha(word, shear=shear)</span><br><span class="line">    prediction = predict_captcha(captcha, net)</span><br><span class="line">    prediction = prediction[:<span class="number">4</span>]</span><br><span class="line">    <span class="keyword">if</span> prediction <span class="keyword">not</span> <span class="keyword">in</span> dictionary:</span><br><span class="line">        distances = sorted([(word, compute_distance(prediction, word))</span><br><span class="line">                            <span class="keyword">for</span> word <span class="keyword">in</span> dictionary],</span><br><span class="line">                           key=itemgetter(<span class="number">1</span>))</span><br><span class="line">        best_word = distances[<span class="number">0</span>]</span><br><span class="line">        prediction = best_word[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> word == prediction, word, prediction</span><br></pre></td></tr></table></figure></li></ul><h2 id="第九章-作者归属问题"><a href="#第九章-作者归属问题" class="headerlink" title="第九章 作者归属问题"></a>第九章 作者归属问题</h2><h3 id="9-1-为作品找作者"><a href="#9-1-为作品找作者" class="headerlink" title="9.1 为作品找作者"></a>9.1 为作品找作者</h3><ul><li><p>作者分析的目标是只根据作品内容找出作者独有的特点，作者分析包括以下问题：</p><ul><li>作者归属：从一组可能的作者中找到文档真正的主人。</li><li>作者画像：根据作品界定作者的年龄、性别或其他特征。</li><li>作者验证：根据作者已有作品，推断其他作品是否也是他写的。</li><li>作者聚类：用聚类分析方法把作品按照作者进行分类。</li></ul></li><li><p>作者归属问题中，已知一部分作者，训练集为多个作者的作品，目标是确定一组作者不详的作品是谁写的。如果作者恰好是已知作者，叫封闭问题。否则叫开放问题。</p></li><li><p>任何数据挖掘问题，若实际类别不在训练集中，则叫开放问题，要给出不属于任何已知类别的提示。</p></li><li><p>进行作者归属研究，要求：</p><ul><li>只能使用作品内容</li><li>不考虑作品主题，关注单词用法、标点和其他文本特征。</li></ul></li><li><p>文档中有很多噪音，比如作品前的声明文字，因此要删去这些噪音。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">clean_book</span><span class="params">(document)</span>:</span></span><br><span class="line">    lines = document.split(<span class="string">"\n"</span>)</span><br><span class="line">    start= <span class="number">0</span></span><br><span class="line">    end = len(lines)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(lines)):</span><br><span class="line">        line = lines[i]</span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">"*** START OF THIS PROJECT GUTENBERG"</span>):</span><br><span class="line">            start = i + <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> line.startswith(<span class="string">"*** END OF THIS PROJECT GUTENBERG"</span>):</span><br><span class="line">            end = i - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">"\n"</span>.join(lines[start:end])</span><br></pre></td></tr></table></figure></li><li><p>将文档清理并保存到列表中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_books_data</span><span class="params">(folder=data_folder)</span>:</span></span><br><span class="line">    documents = []</span><br><span class="line">    authors = []</span><br><span class="line">    subfolders = [subfolder <span class="keyword">for</span> subfolder <span class="keyword">in</span> os.listdir(folder)</span><br><span class="line">                  <span class="keyword">if</span> os.path.isdir(os.path.join(folder, subfolder))]</span><br><span class="line">    <span class="keyword">for</span> author_number, subfolder <span class="keyword">in</span> enumerate(subfolders):</span><br><span class="line">        full_subfolder_path = os.path.join(folder, subfolder)</span><br><span class="line">        <span class="keyword">for</span> document_name <span class="keyword">in</span> os.listdir(full_subfolder_path):</span><br><span class="line">            <span class="keyword">with</span> open(os.path.join(full_subfolder_path, document_name)) <span class="keyword">as</span> inf:</span><br><span class="line">                documents.append(clean_book(inf.read()))</span><br><span class="line">                authors.append(author_number)</span><br><span class="line">    <span class="keyword">return</span> documents, np.array(authors, dtype=<span class="string">'int'</span>)</span><br></pre></td></tr></table></figure></li><li><p>如果数据集过大，无法一次加载到内存中，要每次从一篇或几篇文档中抽取特征，把特征保存在矩阵中。</p></li></ul><h3 id="9-2-功能词"><a href="#9-2-功能词" class="headerlink" title="9.2 功能词"></a>9.2 功能词</h3><ul><li><p>功能词：指本身具有很少含义，却是组成句子必不可少的成分。如this和which。与功能词相对的是实词。</p></li><li><p>通常来讲，使用越频繁的单词，对于作者分析越能提供更多有价值的信息。</p></li><li><p>功能词的使用通常不是由文档内容而是由作者的使用习惯所决定的，因此可以用来区分作者归属，如美国人在意区分that和which，而澳大利亚人不在意。</p></li><li><p>功能词词汇表：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">function_words = [<span class="string">"a"</span>, <span class="string">"able"</span>, <span class="string">"aboard"</span>, <span class="string">"about"</span>, <span class="string">"above"</span>, <span class="string">"absent"</span>,</span><br><span class="line">                  <span class="string">"according"</span> , <span class="string">"accordingly"</span>, <span class="string">"across"</span>, <span class="string">"after"</span>, <span class="string">"against"</span>,</span><br><span class="line">                  <span class="string">"ahead"</span>, <span class="string">"albeit"</span>, <span class="string">"all"</span>, <span class="string">"along"</span>, <span class="string">"alongside"</span>, <span class="string">"although"</span>,</span><br><span class="line">                  <span class="string">"am"</span>, <span class="string">"amid"</span>, <span class="string">"amidst"</span>, <span class="string">"among"</span>, <span class="string">"amongst"</span>, <span class="string">"amount"</span>, <span class="string">"an"</span>,</span><br><span class="line">                    <span class="string">"and"</span>, <span class="string">"another"</span>, <span class="string">"anti"</span>, <span class="string">"any"</span>, <span class="string">"anybody"</span>, <span class="string">"anyone"</span>,</span><br><span class="line">                    <span class="string">"anything"</span>, <span class="string">"are"</span>, <span class="string">"around"</span>, <span class="string">"as"</span>, <span class="string">"aside"</span>, <span class="string">"astraddle"</span>,</span><br><span class="line">                    <span class="string">"astride"</span>, <span class="string">"at"</span>, <span class="string">"away"</span>, <span class="string">"bar"</span>, <span class="string">"barring"</span>, <span class="string">"be"</span>, <span class="string">"because"</span>,</span><br><span class="line">                    <span class="string">"been"</span>, <span class="string">"before"</span>, <span class="string">"behind"</span>, <span class="string">"being"</span>, <span class="string">"below"</span>, <span class="string">"beneath"</span>,</span><br><span class="line">                    <span class="string">"beside"</span>, <span class="string">"besides"</span>, <span class="string">"better"</span>, <span class="string">"between"</span>, <span class="string">"beyond"</span>, <span class="string">"bit"</span>,</span><br><span class="line">                    <span class="string">"both"</span>, <span class="string">"but"</span>, <span class="string">"by"</span>, <span class="string">"can"</span>, <span class="string">"certain"</span>, <span class="string">"circa"</span>, <span class="string">"close"</span>,</span><br><span class="line">                    <span class="string">"concerning"</span>, <span class="string">"consequently"</span>, <span class="string">"considering"</span>, <span class="string">"could"</span>,</span><br><span class="line">                    <span class="string">"couple"</span>, <span class="string">"dare"</span>, <span class="string">"deal"</span>, <span class="string">"despite"</span>, <span class="string">"down"</span>, <span class="string">"due"</span>, <span class="string">"during"</span>,</span><br><span class="line">                    <span class="string">"each"</span>, <span class="string">"eight"</span>, <span class="string">"eighth"</span>, <span class="string">"either"</span>, <span class="string">"enough"</span>, <span class="string">"every"</span>,</span><br><span class="line">                    <span class="string">"everybody"</span>, <span class="string">"everyone"</span>, <span class="string">"everything"</span>, <span class="string">"except"</span>, <span class="string">"excepting"</span>,</span><br><span class="line">                    <span class="string">"excluding"</span>, <span class="string">"failing"</span>, <span class="string">"few"</span>, <span class="string">"fewer"</span>, <span class="string">"fifth"</span>, <span class="string">"first"</span>,</span><br><span class="line">                    <span class="string">"five"</span>, <span class="string">"following"</span>, <span class="string">"for"</span>, <span class="string">"four"</span>, <span class="string">"fourth"</span>, <span class="string">"from"</span>, <span class="string">"front"</span>,</span><br><span class="line">                    <span class="string">"given"</span>, <span class="string">"good"</span>, <span class="string">"great"</span>, <span class="string">"had"</span>, <span class="string">"half"</span>, <span class="string">"have"</span>, <span class="string">"he"</span>,</span><br><span class="line">                    <span class="string">"heaps"</span>, <span class="string">"hence"</span>, <span class="string">"her"</span>, <span class="string">"hers"</span>, <span class="string">"herself"</span>, <span class="string">"him"</span>, <span class="string">"himself"</span>,</span><br><span class="line">                    <span class="string">"his"</span>, <span class="string">"however"</span>, <span class="string">"i"</span>, <span class="string">"if"</span>, <span class="string">"in"</span>, <span class="string">"including"</span>, <span class="string">"inside"</span>,</span><br><span class="line">                    <span class="string">"instead"</span>, <span class="string">"into"</span>, <span class="string">"is"</span>, <span class="string">"it"</span>, <span class="string">"its"</span>, <span class="string">"itself"</span>, <span class="string">"keeping"</span>,</span><br><span class="line">                    <span class="string">"lack"</span>, <span class="string">"less"</span>, <span class="string">"like"</span>, <span class="string">"little"</span>, <span class="string">"loads"</span>, <span class="string">"lots"</span>, <span class="string">"majority"</span>,</span><br><span class="line">                    <span class="string">"many"</span>, <span class="string">"masses"</span>, <span class="string">"may"</span>, <span class="string">"me"</span>, <span class="string">"might"</span>, <span class="string">"mine"</span>, <span class="string">"minority"</span>,</span><br><span class="line">                    <span class="string">"minus"</span>, <span class="string">"more"</span>, <span class="string">"most"</span>, <span class="string">"much"</span>, <span class="string">"must"</span>, <span class="string">"my"</span>, <span class="string">"myself"</span>,</span><br><span class="line">                    <span class="string">"near"</span>, <span class="string">"need"</span>, <span class="string">"neither"</span>, <span class="string">"nevertheless"</span>, <span class="string">"next"</span>, <span class="string">"nine"</span>,</span><br><span class="line">                    <span class="string">"ninth"</span>, <span class="string">"no"</span>, <span class="string">"nobody"</span>, <span class="string">"none"</span>, <span class="string">"nor"</span>, <span class="string">"nothing"</span>,</span><br><span class="line">                    <span class="string">"notwithstanding"</span>, <span class="string">"number"</span>, <span class="string">"numbers"</span>, <span class="string">"of"</span>, <span class="string">"off"</span>, <span class="string">"on"</span>,</span><br><span class="line">                    <span class="string">"once"</span>, <span class="string">"one"</span>, <span class="string">"onto"</span>, <span class="string">"opposite"</span>, <span class="string">"or"</span>, <span class="string">"other"</span>, <span class="string">"ought"</span>,</span><br><span class="line">                    <span class="string">"our"</span>, <span class="string">"ours"</span>, <span class="string">"ourselves"</span>, <span class="string">"out"</span>, <span class="string">"outside"</span>, <span class="string">"over"</span>, <span class="string">"part"</span>,</span><br><span class="line">                    <span class="string">"past"</span>, <span class="string">"pending"</span>, <span class="string">"per"</span>, <span class="string">"pertaining"</span>, <span class="string">"place"</span>, <span class="string">"plenty"</span>,</span><br><span class="line">                    <span class="string">"plethora"</span>, <span class="string">"plus"</span>, <span class="string">"quantities"</span>, <span class="string">"quantity"</span>, <span class="string">"quarter"</span>,</span><br><span class="line">                    <span class="string">"regarding"</span>, <span class="string">"remainder"</span>, <span class="string">"respecting"</span>, <span class="string">"rest"</span>, <span class="string">"round"</span>,</span><br><span class="line">                    <span class="string">"save"</span>, <span class="string">"saving"</span>, <span class="string">"second"</span>, <span class="string">"seven"</span>, <span class="string">"seventh"</span>, <span class="string">"several"</span>,</span><br><span class="line">                    <span class="string">"shall"</span>, <span class="string">"she"</span>, <span class="string">"should"</span>, <span class="string">"similar"</span>, <span class="string">"since"</span>, <span class="string">"six"</span>, <span class="string">"sixth"</span>,</span><br><span class="line">                    <span class="string">"so"</span>, <span class="string">"some"</span>, <span class="string">"somebody"</span>, <span class="string">"someone"</span>, <span class="string">"something"</span>, <span class="string">"spite"</span>,</span><br><span class="line">                    <span class="string">"such"</span>, <span class="string">"ten"</span>, <span class="string">"tenth"</span>, <span class="string">"than"</span>, <span class="string">"thanks"</span>, <span class="string">"that"</span>, <span class="string">"the"</span>,</span><br><span class="line">                    <span class="string">"their"</span>, <span class="string">"theirs"</span>, <span class="string">"them"</span>, <span class="string">"themselves"</span>, <span class="string">"then"</span>, <span class="string">"thence"</span>,</span><br><span class="line">                  <span class="string">"therefore"</span>, <span class="string">"these"</span>, <span class="string">"they"</span>, <span class="string">"third"</span>, <span class="string">"this"</span>, <span class="string">"those"</span>,</span><br><span class="line"><span class="string">"though"</span>, <span class="string">"three"</span>, <span class="string">"through"</span>, <span class="string">"throughout"</span>, <span class="string">"thru"</span>, <span class="string">"thus"</span>,</span><br><span class="line"><span class="string">"till"</span>, <span class="string">"time"</span>, <span class="string">"to"</span>, <span class="string">"tons"</span>, <span class="string">"top"</span>, <span class="string">"toward"</span>, <span class="string">"towards"</span>,</span><br><span class="line"><span class="string">"two"</span>, <span class="string">"under"</span>, <span class="string">"underneath"</span>, <span class="string">"unless"</span>, <span class="string">"unlike"</span>, <span class="string">"until"</span>,</span><br><span class="line"><span class="string">"unto"</span>, <span class="string">"up"</span>, <span class="string">"upon"</span>, <span class="string">"us"</span>, <span class="string">"used"</span>, <span class="string">"various"</span>, <span class="string">"versus"</span>,</span><br><span class="line"><span class="string">"via"</span>, <span class="string">"view"</span>, <span class="string">"wanting"</span>, <span class="string">"was"</span>, <span class="string">"we"</span>, <span class="string">"were"</span>, <span class="string">"what"</span>,</span><br><span class="line"><span class="string">"whatever"</span>, <span class="string">"when"</span>, <span class="string">"whenever"</span>, <span class="string">"where"</span>, <span class="string">"whereas"</span>,</span><br><span class="line"><span class="string">"wherever"</span>, <span class="string">"whether"</span>, <span class="string">"which"</span>, <span class="string">"whichever"</span>, <span class="string">"while"</span>,</span><br><span class="line">                  <span class="string">"whilst"</span>, <span class="string">"who"</span>, <span class="string">"whoever"</span>, <span class="string">"whole"</span>, <span class="string">"whom"</span>, <span class="string">"whomever"</span>,</span><br><span class="line"><span class="string">"whose"</span>, <span class="string">"will"</span>, <span class="string">"with"</span>, <span class="string">"within"</span>, <span class="string">"without"</span>, <span class="string">"would"</span>, <span class="string">"yet"</span>,</span><br><span class="line"><span class="string">"you"</span>, <span class="string">"your"</span>, <span class="string">"yours"</span>, <span class="string">"yourself"</span>, <span class="string">"yourselves"</span>]</span><br></pre></td></tr></table></figure></li><li><p>使用<code>CountVectorizer</code>抽取词频特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">extractor = CountVectorizer(vocabulary=function_words)</span><br></pre></td></tr></table></figure></li><li><p>设置支持向量机参数，创建分类器实例。高斯内核如rbf，只适用于特征数小于10000的情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">parameters = &#123;<span class="string">'kernel'</span>:(<span class="string">'linear'</span>, <span class="string">'rbf'</span>), <span class="string">'C'</span>:[<span class="number">1</span>, <span class="number">10</span>]&#125;</span><br><span class="line">svr = SVC()</span><br></pre></td></tr></table></figure></li><li><p>使用网络搜索法寻找最优参数值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> grid_search</span><br><span class="line">grid = grid_search.GridSearchCV(svr, parameters)</span><br></pre></td></tr></table></figure></li><li><p>组建流水线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pipeline1 = Pipeline([(<span class="string">'feature_extraction'</span>, extractor),</span><br><span class="line">                      (<span class="string">'clf'</span>, grid)</span><br><span class="line">                     ])</span><br></pre></td></tr></table></figure></li></ul><h3 id="9-3-支持向量机"><a href="#9-3-支持向量机" class="headerlink" title="9.3 支持向量机"></a>9.3 支持向量机</h3><ul><li>支持向量机是一种二类分类器。SVM要做的是找到最佳的一条线，分割开两个类别的数据，让各点到分割线之间的距离最大化，用它做预测。</li><li>对于多分类问题，就创建多个分类器，最简单的方法是分为1对多，即特定类和其他类。</li><li><code>from sklearn.svm import SVC</code>的参数：<ul><li>C参数：与分类器正确分类的比例有关，过高可能过拟合，过小分类结果可能较差。</li><li>kernel参数：指定内核函数。如果数据线性不可分，则要加入伪特征将其置入高维空间，直到其线性可分。寻找最佳分割线时，需要计算个体之间的点积，使用点积函数可以创建新特征而无需实际定义这些特征。因此内核函数定义为数据集中两个个体函数的点积。内核有三种：线性内核，多项式内核，高斯内核。</li></ul></li></ul><h3 id="9-4-字符N元语法"><a href="#9-4-字符N元语法" class="headerlink" title="9.4 字符N元语法"></a>9.4 字符N元语法</h3><ul><li><p>N元语法由一系列N个为一组的对象组成，N通常为2到6之间的值。基于字符的N元语法在作者归属问题上效果很好。更常见的是基于单词的N元语法。</p></li><li><p>N元语法的特征如<code>&lt;e t&gt;</code>是由e、空格和t组成的。</p></li><li><p>字符N元语法的特点是稀疏，但低于基于单词的N元语法。</p></li><li><p>抽取N元语法，analyzer指定了抽取字符，ngram_range指定N的范围，取同样长度的N元语法，则使用相同的值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CountVectorizer(analyzer=<span class="string">'char'</span>, ngram_range=(<span class="number">3</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure></li></ul><h3 id="9-5-使用安然公司数据集"><a href="#9-5-使用安然公司数据集" class="headerlink" title="9.5 使用安然公司数据集"></a>9.5 使用安然公司数据集</h3><ul><li><p>初始化邮件解析器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> email.parser <span class="keyword">import</span> Parser</span><br><span class="line">p = Parser()</span><br></pre></td></tr></table></figure></li><li><p>为保证数据集相对平衡，设定发件人最少发件数和最大抽取邮件数。</p></li><li><p>打乱邮箱地址。因listdir每次获取的邮箱顺序不一定相同，所以先排序，再打乱。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">email_addresses = sorted(os.listdir(data_folder))</span><br><span class="line">random_state.shuffle(email_addresses)</span><br></pre></td></tr></table></figure></li><li><p>解析邮件，获取邮件内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">contents = [p.parsestr(email)._payload <span class="keyword">for</span> email <span class="keyword">in</span> authored_emails]</span><br><span class="line">documents.extend(contents)</span><br></pre></td></tr></table></figure></li><li><p>获取安然语料库函数如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_enron_corpus</span><span class="params">(num_authors=<span class="number">10</span>, data_folder=data_folder,</span></span></span><br><span class="line"><span class="function"><span class="params">                     min_docs_author=<span class="number">10</span>, max_docs_author=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     random_state=None)</span>:</span></span><br><span class="line">    random_state = check_random_state(random_state)</span><br><span class="line">    email_addresses = sorted(os.listdir(data_folder))</span><br><span class="line">    random_state.shuffle(email_addresses)</span><br><span class="line">    documents = []</span><br><span class="line">    classes = []</span><br><span class="line">    author_num = <span class="number">0</span></span><br><span class="line">    authors = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> email_addresses:</span><br><span class="line">        users_email_folder = os.path.join(data_folder, user)</span><br><span class="line">        mail_folders = [os.path.join(users_email_folder, subfolder)</span><br><span class="line">                        <span class="keyword">for</span> subfolder <span class="keyword">in</span> os.listdir(users_email_folder)</span><br><span class="line">                        <span class="keyword">if</span> <span class="string">"sent"</span> <span class="keyword">in</span> subfolder]</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            authored_emails = [open(os.path.join(mail_folder, email_filename), encoding=<span class="string">'cp1252'</span>).read()</span><br><span class="line">                               <span class="keyword">for</span> mail_folder <span class="keyword">in</span> mail_folders</span><br><span class="line">                               <span class="keyword">for</span> email_filename <span class="keyword">in</span> os.listdir(mail_folder)]</span><br><span class="line">        <span class="keyword">except</span> IsADirectoryError:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> len(authored_emails) &lt; min_docs_author:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> len(authored_emails) &gt; max_docs_author:</span><br><span class="line">            authored_emails = authored_emails[:max_docs_author]</span><br><span class="line">        contents = [p.parsestr(email)._payload <span class="keyword">for</span> email <span class="keyword">in</span> authored_emails]</span><br><span class="line">        documents.extend(contents)</span><br><span class="line">        classes.extend([author_num] * len(authored_emails))</span><br><span class="line">        authors[user] = author_num</span><br><span class="line">        author_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> author_num &gt;= num_authors <span class="keyword">or</span> author_num &gt;= len(email_addresses):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> documents, np.array(classes), authors</span><br></pre></td></tr></table></figure></li><li><p>由于回复邮件时会带上别人之前邮件的内容，因此要进行处理。</p></li><li><p>使用<code>import quotequail</code>查找邮件中的新内容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_replies</span><span class="params">(email_contents)</span>:</span></span><br><span class="line">    r = quotequail.unwrap(email_contents)</span><br><span class="line">    <span class="keyword">if</span> r <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> email_contents</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'text_top'</span> <span class="keyword">in</span> r:</span><br><span class="line">        <span class="keyword">return</span> r[<span class="string">'text_top'</span>]</span><br><span class="line">    <span class="keyword">elif</span> <span class="string">'text'</span> <span class="keyword">in</span> r:</span><br><span class="line">        <span class="keyword">return</span> r[<span class="string">'text'</span>]</span><br><span class="line">    <span class="keyword">return</span> email_contents</span><br></pre></td></tr></table></figure></li><li><p>线上学习：使用新数据更新训练结果，但不是每次都重新进行训练。</p></li><li><p>输出最佳训练参数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(pipeline.named_steps[<span class="string">'classifier'</span>].best_params_)</span><br></pre></td></tr></table></figure></li><li><p>创建混淆矩阵，获取发件人。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line">cm = confusion_matrix(y_pred, y_test)</span><br><span class="line">cm = cm / cm.astype(np.float).sum(axis=<span class="number">1</span>)</span><br><span class="line">sorted_authors = sorted(authors.keys(), key=<span class="keyword">lambda</span> x:authors[x])</span><br></pre></td></tr></table></figure></li></ul><h2 id="第十章-新闻预料分类"><a href="#第十章-新闻预料分类" class="headerlink" title="第十章 新闻预料分类"></a>第十章 新闻预料分类</h2><h3 id="10-1-获取新闻文章"><a href="#10-1-获取新闻文章" class="headerlink" title="10.1 获取新闻文章"></a>10.1 获取新闻文章</h3><ul><li><p>已知目标类别的学习任务叫有监督学习，未知目标类别的学习任务叫无监督学习。</p></li><li><p>使用WEB API采集数据，如使用twitterAPI采集数据，有三个注意事项：</p><ul><li>授权方法：是数据提供方用来管理数据采集方的。</li><li>采集频率：限制了采集方在约定时间内的最大请求数。</li><li>API端点：用来抽取信息的实际网址。</li></ul></li><li><p>获取信息时发送HTTP GET请求到指定网址。服务器返回资源信息、信息类型和ID。</p></li><li><p>从reddit上创建script型应用，获得client ID和密钥。</p></li><li><p>设置唯一用户代理，避免与其他API重复，影响采集限制。<code>USER_AGENT = &quot;python:&lt;unique user agent&gt; (by /u/&lt;reddit username&gt;)&quot;</code></p></li><li><p>登录获取令牌。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login</span><span class="params">(username, password)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> password <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        password = getpass.getpass(<span class="string">"Enter reddit password for username &#123;&#125;:"</span>.format(username))</span><br><span class="line">    headers = &#123;<span class="string">"User-Agent"</span>: USER_AGENT&#125;</span><br><span class="line">    client_auth = requests.auth.HTTPBasicAuth(CLIENT_ID, CLIENT_SECRET)</span><br><span class="line">    post_data = &#123;<span class="string">"grant_type"</span>: <span class="string">"password"</span>, <span class="string">"username"</span>:username, <span class="string">"password"</span>:password&#125;</span><br><span class="line">    response = requests.post(<span class="string">"https://www.reddit.com/api/v1/access_token"</span>, auth=client_auth, data=post_data, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> response.json()</span><br></pre></td></tr></table></figure></li><li><p>指定reddit栏目搜集信息。设置头部。获取返回的信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">subreddit = <span class="string">"worldnews"</span></span><br><span class="line">url = <span class="string">"https://oauth.reddit.com/r/&#123;&#125;"</span>.format(subreddit)</span><br><span class="line">headers = &#123;<span class="string">"Authorization"</span>: <span class="string">"bearer &#123;&#125;"</span>.format(token[<span class="string">'access_token'</span>]), <span class="string">"User-Agent"</span>: USER_AGENT&#125;</span><br><span class="line">response = requests.get(url,headers=headers)</span><br></pre></td></tr></table></figure></li><li><p>输出每条广播的标题。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">result = response.json()</span><br><span class="line"><span class="keyword">for</span> story <span class="keyword">in</span> result[<span class="string">'data'</span>][<span class="string">'children'</span>]:</span><br><span class="line">    print(story[<span class="string">'data'</span>][<span class="string">'title'</span>])</span><br></pre></td></tr></table></figure></li><li><p>获取500条广播的标题、连接和喜欢数。因为每页最多是100条广播，因此要用游标，reddit的游标是after。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_links</span><span class="params">(subreddit, token, n_pages=<span class="number">5</span>)</span>:</span></span><br><span class="line">    stories = []</span><br><span class="line">    after = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> page_number <span class="keyword">in</span> range(n_pages):</span><br><span class="line">        headers = &#123;<span class="string">"Authorization"</span>: <span class="string">"bearer &#123;&#125;"</span>.format(token[<span class="string">'access_token'</span>]), <span class="string">"User-Agent"</span>: USER_AGENT&#125;</span><br><span class="line">        url = <span class="string">"https://oauth.reddit.com/r/&#123;&#125;?limit=100"</span>.format(subreddit)</span><br><span class="line">        <span class="keyword">if</span> after:</span><br><span class="line">            url += <span class="string">"&amp;after=&#123;&#125;"</span>.format(after)</span><br><span class="line">        response = requests.get(url,headers=headers)</span><br><span class="line">        result = response.json()</span><br><span class="line">        after = result[<span class="string">'data'</span>][<span class="string">'after'</span>]</span><br><span class="line">        sleep(<span class="number">2</span>)</span><br><span class="line">        stories.extend([(story[<span class="string">'data'</span>][<span class="string">'title'</span>], story[<span class="string">'data'</span>][<span class="string">'url'</span>], story[<span class="string">'data'</span>][<span class="string">'score'</span>]) <span class="keyword">for</span> story <span class="keyword">in</span> result[<span class="string">'data'</span>][<span class="string">'children'</span>]])</span><br><span class="line">    <span class="keyword">return</span> stories</span><br></pre></td></tr></table></figure></li></ul><h3 id="10-2-从任意网站抽取文本"><a href="#10-2-从任意网站抽取文本" class="headerlink" title="10.2 从任意网站抽取文本"></a>10.2 从任意网站抽取文本</h3><ul><li><p>使用中文系统中的txt文件默认编码为gbk，要改成utf8，否则大量英文信息无法正确编码。爬取连接信息时，要加上头部，以免被识别为爬虫而无法正常返回信息。reddit标题不唯一，因此使用md5获取散列值作为文件名，md5在小规模数据中是可靠的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line">data_floder = <span class="string">"raw/"</span></span><br><span class="line">number_errors = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> title, url, score <span class="keyword">in</span> stories:</span><br><span class="line">    output_filename = hashlib.md5(url.encode()).hexdigest()</span><br><span class="line">    fullpath = os.path.join(data_floder, output_filename + <span class="string">".txt"</span>)</span><br><span class="line">    headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0(compatible; MSIE 5.5; Windows NT)'</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = requests.get(url, headers=headers)</span><br><span class="line">        data = response.text</span><br><span class="line">        <span class="keyword">with</span> codecs.open(fullpath, <span class="string">'w'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> outf:</span><br><span class="line">            outf.write(data)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        number_errors += <span class="number">1</span></span><br><span class="line">        print(e)</span><br></pre></td></tr></table></figure></li><li><p>使用lxml解析HTML文件，lxml的HTML解析器容错能力强，可以处理不规范的HTML代码。</p></li><li><p>文本抽取分三步：</p><ul><li>遍历HTML文件的每个节点，抽取其中的文本内容。</li><li>跳过JavaScript、样式和注释节点。</li><li>确保文本内容长度至少为100个字符。</li></ul></li><li><p>遍历解析树，拼接获取文本。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text_from_node</span><span class="params">(node)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(node) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">if</span> node.text <span class="keyword">and</span> len(node.text) &gt; <span class="number">100</span>:</span><br><span class="line">            <span class="keyword">return</span> node.text</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="string">""</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        results = (get_text_from_node(child) <span class="keyword">for</span> child <span class="keyword">in</span> node <span class="keyword">if</span> child.tag <span class="keyword">not</span> <span class="keyword">in</span> skip_node_types)</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"\n"</span>.join(r <span class="keyword">for</span> r <span class="keyword">in</span> results <span class="keyword">if</span> len(r) &gt; <span class="number">1</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_text_from_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> codecs.open(filename, encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> inf:</span><br><span class="line">        html_tree = etree.parse(inf, etree.HTMLParser())</span><br><span class="line">    <span class="keyword">return</span> get_text_from_node(html_tree.getroot())</span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(data_floder):</span><br><span class="line">    text = get_text_from_file(os.path.join(data_floder, filename))</span><br><span class="line">    <span class="keyword">with</span> codecs.open(os.path.join(text_output_folder, filename), <span class="string">'w'</span>, <span class="string">'utf8'</span>) <span class="keyword">as</span> outf:</span><br><span class="line">        outf.write(text)</span><br></pre></td></tr></table></figure></li></ul><h3 id="10-3-新闻预料分类"><a href="#10-3-新闻预料分类" class="headerlink" title="10.3 新闻预料分类"></a>10.3 新闻预料分类</h3><ul><li><p>聚类算法在学习时没有明确的方向性，根据目标函数而不是数据潜在的含义学习。因此聚类算法选择效果好的特征很重要。有监督学习中，算法会自动降低对分类作用不大的特征的权重，而聚类会综合所有特征给出最后结果。</p></li><li><p>k-means聚类算法迭代寻找能够代表数据的聚类质心点。算法开始时使用从训练数据中随机选取的k个数据点作为质心。在迭代一定次数后，质心移动量很小时，可以终止算法的运行。步骤如下：</p><ul><li>为每一个数据点分配簇标签，标签根据与各质心的距离选取。</li><li>计算各簇内所有数据点均值，更新各簇的质心点。</li></ul></li><li><p><code>from sklearn.cluster import KMeans</code>使用kmeans聚类算法。</p></li><li><p><code>from sklearn.feature_extraction.text import TfidfVectorizer</code>引入抽取tf-idf特征的向量化工具。</p></li><li><p>封装流水线，设定max_df=0.4忽略在40%以上文档中出现过的词语。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line">n_clusters = <span class="number">10</span></span><br><span class="line">pipeline = Pipeline([(<span class="string">'feature_extraction'</span>, TfidfVectorizer(max_df=<span class="number">0.4</span>)),</span><br><span class="line">                     (<span class="string">'clusterer'</span>, KMeans(n_clusters=n_clusters))</span><br><span class="line">                     ])</span><br></pre></td></tr></table></figure></li><li><p>使用Counter函数计算每类数据点个数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">c = Counter(labels)</span><br><span class="line"><span class="keyword">for</span> cluster_number <span class="keyword">in</span> range(n_clusters):</span><br><span class="line">    print(<span class="string">"Cluster &#123;&#125; contains &#123;&#125; samples"</span>.format(cluster_number, c[cluster_number]))</span><br></pre></td></tr></table></figure></li><li><p>聚类算法是探索性算法，很难评估算法结果的好坏，评估最直接的方式是根据其学习的标准进行评价。</p></li><li><p>计算kmeans算法的惯性权重即每个数据点到最近质心点的距离，这个值本身没有意义，但可以用来判断分多少簇合适。</p></li><li><p>对于每个簇数计算30次。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">inertia_scores = []</span><br><span class="line">n_cluster_values = list(range(2, 20))</span><br><span class="line">for n_clusters in n_cluster_values:</span><br><span class="line">    cur_inertia_scores = []</span><br><span class="line">    X = TfidfVectorizer(max_df=0.4).fit_transform(documents)</span><br><span class="line">    for i in range(30):</span><br><span class="line">        km = KMeans(n_clusters=n_clusters).fit(X)</span><br><span class="line">        cur_inertia_scores.append(km.inertia_)</span><br><span class="line">    inertia_scores.append(cur_inertia_scores)</span><br><span class="line">inertia_scores = np.array(inertia_scores)</span><br></pre></td></tr></table></figure></li><li><p>计算均值和标准差，画出图像。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">inertia_means = np.mean(inertia_scores, axis=<span class="number">1</span>)</span><br><span class="line">inertia_stderr = np.std(inertia_scores, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">40</span>,<span class="number">20</span>))</span><br><span class="line">plt.errorbar(n_cluster_values, inertia_means, inertia_stderr, color=<span class="string">'green'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></li><li><p>随着簇增加，惯性权重逐渐减少，但当簇数为kt时，惯性权重最后进行了一次大的调整，如同图像的肘部，称为拐点。有的数据集拐点明显，有的数据集则没有拐点。</p></li><li><p>从质心找出特征值最大的5个特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"  Most important terms"</span>)</span><br><span class="line">    centroid = pipeline.named_steps[<span class="string">'clusterer'</span>].cluster_centers_[cluster_number]</span><br><span class="line">    most_important = centroid.argsort()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">        term_index = most_important[-(i+<span class="number">1</span>)]</span><br><span class="line">        print(<span class="string">"  &#123;0&#125;) &#123;1&#125; (score: &#123;2:.4f&#125;)"</span>.format(i+<span class="number">1</span>, terms[term_index], centroid[term_index]))</span><br></pre></td></tr></table></figure></li><li><p>k聚类算法可用来简化特征，其他特征简化方法如主成分分析、潜在语义索引的计算要求很高。使用数据点到质心点的距离作为特征，来简化特征。</p></li><li><p>简化特征后可以进行二次聚类。</p></li><li><p>分类时也可使用聚类来简化特征：</p><ul><li>使用标注好的数据选取特征</li><li>用聚类方法简化特征</li><li>用分类算法对前面处理好的数据分类</li></ul></li></ul><h3 id="10-4-聚类融合"><a href="#10-4-聚类融合" class="headerlink" title="10.4 聚类融合"></a>10.4 聚类融合</h3><ul><li><p>聚类融合后的算法能够平滑算法多次运行得到的不同结果，也可以减少参数选择对于最终结果的影响。</p></li><li><p>证据累积算法：对数据多次聚类，每次都记录各个数据点的簇标签，计算每两个数据点被分到同一个簇的次数。步骤如下：</p><ul><li>使用kmeans等低水平聚类算法对数据集进行多次聚类，记录每一次迭代两个数据点出现在同一簇的频率，将结果保存到共协矩阵。</li><li>使用分级聚类对第一步得到的共协矩阵进行聚类分析。分级聚类等价于找到一棵把所有节点连接到一起的树，并把权重低的边去掉。</li></ul></li><li><p><code>from scipy.sparse import csr_matrix</code>使用scipy的稀疏矩阵csr_matrix。稀疏矩阵由一系列记录非零值位置的列表组成。</p></li><li><p>创建共协矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_coassociation_matrix</span><span class="params">(labels)</span>:</span></span><br><span class="line">    rows = []</span><br><span class="line">    cols = []</span><br><span class="line">    unique_labels = set(labels)</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> unique_labels:</span><br><span class="line">        indices = np.where(labels == label)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">for</span> index1 <span class="keyword">in</span> indices:</span><br><span class="line">            <span class="keyword">for</span> index2 <span class="keyword">in</span> indices:</span><br><span class="line">                rows.append(index1)</span><br><span class="line">                cols.append(index2)</span><br><span class="line">    data = np.ones((len(rows),))</span><br><span class="line">    <span class="keyword">return</span> csr_matrix((data, (rows, cols)), dtype=<span class="string">'float'</span>)</span><br></pre></td></tr></table></figure></li><li><p>分级聚类即找到该矩阵的最小生成树，删除权重低于阈值的边。</p></li><li><p>生成树是所有节点都连接到一起的树。</p></li><li><p>最小生成树是总权重最低的生成树。</p></li><li><p>图中的节点是数据集中的个体，边是被分到同一簇的次数即共协矩阵的值。</p></li><li><p><code>from scipy.sparse.csgraph import minimum_spanning_tree</code>使用scipy中的minimum_spanning_tree计算最小生成树。<code>mst = minimum_spanning_tree(C)</code>，函数输入为距离，因此要取反。</p></li><li><p>再次遍历，得到第二次聚类的共协矩阵，删除不是两个共协矩阵中都出现的边。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pipeline = Pipeline([(<span class="string">'feature_extraction'</span>, TfidfVectorizer(max_df=<span class="number">0.4</span>)),</span><br><span class="line">                     (<span class="string">'clusterer'</span>, KMeans(n_clusters=<span class="number">3</span>))</span><br><span class="line">                     ])</span><br><span class="line">pipeline.fit(documents)</span><br><span class="line">labels2 = pipeline.predict(documents)</span><br><span class="line">C2 = create_coassociation_matrix(labels2)</span><br><span class="line">C_sum = (C + C2) / <span class="number">2</span></span><br><span class="line">C_sum.todense()</span><br><span class="line">mst = minimum_spanning_tree(-C_sum)</span><br><span class="line">mst.data[mst.data &gt; <span class="number">-1</span>] = <span class="number">0</span></span><br><span class="line">mst.eliminate_zeros()</span><br></pre></td></tr></table></figure></li><li><p>找到所有连通分支。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.sparse.csgraph <span class="keyword">import</span> connected_components</span><br><span class="line">number_of_clusters, labels = connected_components(mst)</span><br></pre></td></tr></table></figure></li><li><p>kmeans算法假定所有特征取值范围相同，找的是圆形簇。当簇不是圆形的时，用kmeans聚类有难度。</p></li><li><p>证据累积算法把特征重新映射到新空间，证据累积算法只关心数据点之间的距离而不是原先在特征空间的位置。但仍需进行数据规范化。</p></li><li><p>指定n_clusterings次聚类进行融合，删除边的阈值为cut_threshold，每次聚类簇的范围为n_clusters_range。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, ClusterMixin</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EAC</span><span class="params">(BaseEstimator, ClusterMixin)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n_clusterings=<span class="number">10</span>, cut_threshold=<span class="number">0.5</span>, n_clusters_range=<span class="params">(<span class="number">3</span>, <span class="number">10</span>)</span>)</span>:</span></span><br><span class="line">        self.n_clusterings = n_clusterings</span><br><span class="line">        self.cut_threshold = cut_threshold</span><br><span class="line">        self.n_clusters_range = n_clusters_range</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        C = sum((create_coassociation_matrix(self._single_clustering(X))</span><br><span class="line">                 <span class="keyword">for</span> i <span class="keyword">in</span> range(self.n_clusterings)))</span><br><span class="line">        mst = minimum_spanning_tree(-C)</span><br><span class="line">        mst.data[mst.data &gt; -self.cut_threshold] = <span class="number">0</span></span><br><span class="line">        mst.eliminate_zeros()</span><br><span class="line">        self.n_components, self.labels_ = connected_components(mst)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_single_clustering</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        n_clusters = np.random.randint(*self.n_clusters_range)</span><br><span class="line">        km = KMeans(n_clusters=n_clusters)</span><br><span class="line">        <span class="keyword">return</span> km.fit_predict(X)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit_predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        self.fit(X)</span><br><span class="line">        <span class="keyword">return</span> self.labels_</span><br></pre></td></tr></table></figure></li><li><p>组成流水线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pipeline = Pipeline([(<span class="string">'feature_extraction'</span>, TfidfVectorizer(max_df=<span class="number">0.4</span>)),</span><br><span class="line">                     (<span class="string">'clusterer'</span>, EAC())</span><br><span class="line">                     ])</span><br></pre></td></tr></table></figure></li></ul><h3 id="10-5-线上学习"><a href="#10-5-线上学习" class="headerlink" title="10.5 线上学习"></a>10.5 线上学习</h3><ul><li><p>当没有足够数据用来训练，或内存不能一次装下所有数据，或完成预测后得到了新的数据，此时可以使用线上学习。</p></li><li><p>线上学习是指用新数据增量地改进模型。神经网络是支持线上学习的标准例子。</p></li><li><p>神经网络也支持使用批模式进行训练，每次只使用一组数据进行训练，运行速度快但耗内存多。</p></li><li><p>线上学习与流式学习有关，不同点在于：线上学习能重新评估先前创建模型时所用的数据，但后者的数据只能用一次。</p></li><li><p><code>from sklearn.cluster import MiniBatchKMeans</code>支持线上学习，实现了partial_fit函数进行线上学习，而fit函数则会删除之前的训练结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mbkm = MiniBatchKMeans(random_state=<span class="number">14</span>, n_clusters=<span class="number">3</span>)</span><br><span class="line">batch_size = <span class="number">500</span></span><br><span class="line"></span><br><span class="line">indices = np.arange(<span class="number">0</span>, X.shape[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    sample = np.random.choice(indices, size=batch_size, replace=<span class="literal">True</span>)</span><br><span class="line">    mbkm.partial_fit(X[sample[:batch_size]])</span><br></pre></td></tr></table></figure></li><li><p>由于TfidfVectorizer不是线上学习算法，所以改用<code>from sklearn.feature_extraction.text import HashingVectorizer</code>，使用散列值代替特征名称，记录词袋模型。</p></li><li><p>创建支持线上学习的pipeline类。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PartialFitPipeline</span><span class="params">(Pipeline)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">partial_fit</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        Xt = X</span><br><span class="line">        <span class="keyword">for</span> name, transform <span class="keyword">in</span> self.steps[:<span class="number">-1</span>]:</span><br><span class="line">            Xt = transform.transform(Xt)</span><br><span class="line">        <span class="keyword">return</span> self.steps[<span class="number">-1</span>][<span class="number">1</span>].partial_fit(Xt, y=y)</span><br></pre></td></tr></table></figure></li><li><p>组装成流水线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pipeline = PartialFitPipeline([(<span class="string">'feature_extraction'</span>, HashingVectorizer()),</span><br><span class="line">                             (<span class="string">'clusterer'</span>, MiniBatchKMeans(random_state=<span class="number">14</span>, n_clusters=<span class="number">3</span>))</span><br><span class="line">                             ])</span><br></pre></td></tr></table></figure></li><li><p>用批模式训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> iteration <span class="keyword">in</span> range(int(len(documents) / batch_size)):</span><br><span class="line">    start = batch_size * iteration</span><br><span class="line">    end = batch_size * (iteration + <span class="number">1</span>)</span><br><span class="line">    pipeline.partial_fit(documents[start:end])</span><br></pre></td></tr></table></figure></li></ul><h2 id="第十一章-用深度学习方法为图像中的物体进行分类"><a href="#第十一章-用深度学习方法为图像中的物体进行分类" class="headerlink" title="第十一章 用深度学习方法为图像中的物体进行分类"></a>第十一章 用深度学习方法为图像中的物体进行分类</h2><h3 id="11-1-应用场景和目标"><a href="#11-1-应用场景和目标" class="headerlink" title="11.1 应用场景和目标"></a>11.1 应用场景和目标</h3><ul><li><p>使用CIFAR-10数据集进行训练，所用图像均为numpy数组。</p></li><li><p>图像数据格式为pickle，pickle是保存图形对象的一个库，调用<code>pickle.load</code>读取数据。编码设置为Latin，防止不同版本python导致的编码错误。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unpickle</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'rb'</span>) <span class="keyword">as</span> fo:</span><br><span class="line">        <span class="keyword">return</span> pickle.load(fo, encoding=<span class="string">'latin1'</span>)</span><br></pre></td></tr></table></figure></li><li><p>将列表数据转换成能用matplotlib绘制的图像，并旋转图片。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">image = image.reshape((<span class="number">32</span>,<span class="number">32</span>, <span class="number">3</span>), order=<span class="string">'F'</span>)</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">image = np.rot90(image, <span class="number">-1</span>)</span><br></pre></td></tr></table></figure></li></ul><h3 id="11-2-深度神经网络"><a href="#11-2-深度神经网络" class="headerlink" title="11.2 深度神经网络"></a>11.2 深度神经网络</h3><ul><li><p>至少包含两层隐含层的神经网络被称为深度神经网络。更巧妙的算法能减少实际需要的层数。</p></li><li><p>神经网络接收很基础的特征作为输入，就计算机视觉而言，输入为简单的像素值。经过神经网络，基础的特征组合成复杂的特征。</p></li><li><p>一个神经网络可以用一组矩阵表示，每层增加一个偏置项，永远激活并与下一层的每个神经元都有连接。</p></li><li><p>Theano是用来创建和运行数学表达式的工具。和SQL相似，在Theano中只需定义要做什么要不是怎么做。</p></li><li><p>Theano用来定义函数，处理标量、数组和矩阵及其他数学表达式。</p></li><li><p>引入张量。定义两个标量数值型输入。构成表达式。定义计算表达式的函数。要注意theano和numpy包的兼容。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> theano</span><br><span class="line"><span class="keyword">from</span> theano <span class="keyword">import</span> tensor <span class="keyword">as</span> T</span><br><span class="line">a = T.dscalar()</span><br><span class="line">b = T.dscalar()</span><br><span class="line">c = T.sqrt(a ** <span class="number">2</span> + b ** <span class="number">2</span>)</span><br><span class="line">f = theano.function([a,b], c)</span><br></pre></td></tr></table></figure></li><li><p>Lasagne库基于Theano库，专门用来构建神经网络，使用Theano 进行计算。实现了几种比较新的神经网络层和组成这些层的模块：</p><ul><li>内置网络层：这些小神经网络比传统神经网络更容易解释。</li><li>删除层：训练过程随机删除神经元，防止产生过拟合问题。</li><li>噪音层：为神经元引入噪音，防止过拟合。</li></ul></li><li><p>卷积层使用少量相互连接的神经元，分析有一部分输入值，便于神经网络实现对数据的标准转换。</p></li><li><p>传统神经网络一层所有神经元全都连接到下一层所有神经元。</p></li><li><p>池化层接收某个区域最大输出值，可以降低图像中的微小变动带来的噪音，减少信息量，减少后续各层的工作量。</p></li><li><p>Lasagene对数据类型有要求，将数据类型转为32位。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data.astype(np.float32)</span><br><span class="line">y_true = iris.target.astype(np.int32)</span><br></pre></td></tr></table></figure></li><li><p>创建输入层，指定每一批输入数量为10，神经元数量和特征数量相同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">input_layer = lasagne.layers.InputLayer(shape=X_train.shape, input_var=input_val)</span><br></pre></td></tr></table></figure></li><li><p>创建隐含层，从输入层接收输入，指定神经元数量，使用非线性sigmoid函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hidden_layer = lasagne.layers.DenseLayer(input_layer, num_units=<span class="number">12</span>, nonlinearity=lasagne.nonlinearities.sigmoid)</span><br></pre></td></tr></table></figure></li><li><p>创建输出层，输出层共三个神经元与类别数一致，使用非线性softmax函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">output_layer = lasagne.layers.DenseLayer(hidden_layer, num_units=<span class="number">3</span>, nonlinearity=lasagne.nonlinearities.softmax)</span><br></pre></td></tr></table></figure></li><li><p>Lasagne中，输入数据先提交到输出层，再向上回溯，直到输入层，将数据交给输入层处理。</p></li><li><p>定义输入、输出、目标数据变量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> lasagne</span><br><span class="line">input_val = T.fmatrix(<span class="string">"inputs"</span>)</span><br><span class="line">target_val = T.ivector(<span class="string">"targets"</span>)</span><br><span class="line">output_val = lasagne.layers.get_output(output_layer)</span><br></pre></td></tr></table></figure></li><li><p>定义损失函数，训练神经网络时以最小化损失函数为前提。使用交叉熵表示损失，这是一种衡量分类数据分类效果好坏的标准。损失函数表示实际网络输出与期望输出之间的差距。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = lasagne.objectives.categorical_crossentropy(output_val, target_val)</span><br><span class="line">loss = loss.mean()</span><br></pre></td></tr></table></figure></li><li><p>获取所有参数，调整网络权重，使损失降到最小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_params = lasagne.layers.get_all_params(output_layer, trainable=<span class="literal">True</span>)</span><br><span class="line">updates = lasagne.updates.sgd(loss, all_params, learning_rate=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></li><li><p>定义训练函数和获取输出的函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = theano.function([input_val, target_val], loss, updates=updates, allow_input_downcast=<span class="literal">True</span>)</span><br><span class="line">get_output = theano.function([input_val], output_val)</span><br></pre></td></tr></table></figure></li><li><p>进行1000次迭代，逐渐改进神经网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    train(X_train, y_train)</span><br></pre></td></tr></table></figure></li><li><p>获取测试集的输出结果及各神经元激励作用的大小，找到激励作用最大的神经元，得到预测结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">y_output = get_output(X_test)</span><br><span class="line">y_pred = np.argmax(y_output, axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">print(f1_score(y_test, y_pred, average=<span class="string">'micro'</span>))</span><br></pre></td></tr></table></figure></li><li><p>nolearn对Lasagne实现了封装，可读性更强，更易管理。</p></li><li><p>创建由输入层、密集隐含层和密集输出层组成的层级结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lasagne <span class="keyword">import</span> layers</span><br><span class="line">layers=[</span><br><span class="line">    (<span class="string">'input'</span>, layers.InputLayer),</span><br><span class="line">    (<span class="string">'hidden'</span>, layers.DenseLayer),</span><br><span class="line">    (<span class="string">'output'</span>, layers.DenseLayer),</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li><li><p>定义神经网络，输入神经网络参数，定义非线性函数，指定偏置神经元。偏置神经元激活后可以对问题做更有针对性的训练，以消除训练中的偏差。定义神经网络训练方式，这里使用低冲量值和高学习速率。将分类问题定义为回归问题，因为输出是数值，所以定义为回归问题更好。最大训练步数设为1000。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">net1 = NeuralNet(layers=layers,</span><br><span class="line">                input_shape=X.shape,</span><br><span class="line">                hidden_num_units=<span class="number">100</span>,</span><br><span class="line">                output_num_units=<span class="number">26</span>,</span><br><span class="line">                hidden_nonlinearity=sigmoid,</span><br><span class="line">                 output_nonlinearity=softmax,</span><br><span class="line">                 hidden_b=np.zeros((<span class="number">100</span>,), dtype=np.float64),</span><br><span class="line">                 update=updates.momentum,</span><br><span class="line">                 update_learning_rate=<span class="number">0.9</span>,</span><br><span class="line">                 update_momentum=<span class="number">0.1</span>,</span><br><span class="line">                 regression=<span class="literal">True</span>,</span><br><span class="line">                 max_epochs=<span class="number">1000</span>,</span><br><span class="line">                )</span><br></pre></td></tr></table></figure></li><li><p>在训练集上训练网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net1.fit(X_train, y_train)</span><br></pre></td></tr></table></figure></li><li><p>评估训练得到的网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_pred = net1.predict(X_test)</span><br><span class="line">y_pred = y_pred.argmax(axis=<span class="number">1</span>)</span><br><span class="line"><span class="keyword">assert</span> len(y_pred) == len(X_test)</span><br><span class="line"><span class="keyword">if</span> len(y_test.shape) &gt; <span class="number">1</span>:</span><br><span class="line">    y_test = y_test.argmax(axis=<span class="number">1</span>)</span><br><span class="line">print(f1_score(y_test, y_pred, average=<span class="string">'macro'</span>))</span><br></pre></td></tr></table></figure></li></ul><h3 id="11-3-GPU优化"><a href="#11-3-GPU优化" class="headerlink" title="11.3 GPU优化"></a>11.3 GPU优化</h3><ul><li>使用稀疏矩阵可以将整个神经网络装进内存。</li><li>神经网络最核心的计算类型是浮点运算，矩阵操作的大量运算可以并行处理。GPU拥有成千上万个小核，适合并行任务，CPU单核工作速度更快，访问内存效率更高，适合序列化任务。所以用GPU进行计算能够提升训练速度。</li></ul><h3 id="11-4-应用"><a href="#11-4-应用" class="headerlink" title="11.4 应用"></a>11.4 应用</h3><ul><li><p>保留像素结构即行列号，把所有批次图像文件名存储到列表中。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">batches = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">6</span>):</span><br><span class="line">    batch_filename = os.path.join(data_folder, <span class="string">"data_batch_&#123;&#125;"</span>.format(i))</span><br><span class="line">    batches.append(unpickle(batch1_filename))</span><br></pre></td></tr></table></figure></li><li><p>纵向添加每批次数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = np.vstack([batch[<span class="string">'data'</span>] <span class="keyword">for</span> batch <span class="keyword">in</span> batches])</span><br></pre></td></tr></table></figure></li><li><p>像素值归一化，并转化为32位浮点数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = np.array(X) / X.max()</span><br><span class="line">X = X.astype(np.float32)</span><br></pre></td></tr></table></figure></li><li><p>纵向添加标签数据，转化为一位有效码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line">y = np.hstack(batch[<span class="string">'labels'</span>] <span class="keyword">for</span> batch <span class="keyword">in</span> batches).flatten()</span><br><span class="line">y = OneHotEncoder().fit_transform(y.reshape(y.shape[<span class="number">0</span>],<span class="number">1</span>)).todense()</span><br><span class="line">y = y.astype(np.float32)</span><br></pre></td></tr></table></figure></li><li><p>划分训练集、测试集，调整数组形状以保留原始图像的数据结构。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>) </span><br><span class="line">X_train = X_train.reshape(<span class="number">-1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br><span class="line">X_test = X_test.reshape(<span class="number">-1</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>)</span><br></pre></td></tr></table></figure></li><li><p>创建神经网络各层。输入层数据与数据集同型。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> lasagne <span class="keyword">import</span> layers</span><br><span class="line">layers=[</span><br><span class="line">        (<span class="string">'input'</span>, layers.InputLayer),</span><br><span class="line">        (<span class="string">'conv1'</span>, layers.Conv2DLayer),</span><br><span class="line">        (<span class="string">'pool1'</span>, layers.MaxPool2DLayer),</span><br><span class="line">        (<span class="string">'conv2'</span>, layers.Conv2DLayer),</span><br><span class="line">        (<span class="string">'pool2'</span>, layers.MaxPool2DLayer),</span><br><span class="line">        (<span class="string">'conv3'</span>, layers.Conv2DLayer),</span><br><span class="line">        (<span class="string">'pool3'</span>, layers.MaxPool2DLayer),</span><br><span class="line">        (<span class="string">'hidden4'</span>, layers.DenseLayer),</span><br><span class="line">        (<span class="string">'hidden5'</span>, layers.DenseLayer),</span><br><span class="line">        (<span class="string">'output'</span>, layers.DenseLayer),</span><br><span class="line">        ]</span><br></pre></td></tr></table></figure></li><li><p>创建神经网络。指定输入数据形状，和数据集形状一致，None表示每次使用默认数量图像数据进行训练。设置卷积层大小及卷积窗口大小。设置池化窗口大小。设置隐含层和输出层大小，输出层大小和类别数量一致。输出层设置非线性函数softmax。设置学习速率和冲量，随着数据量的增加，学习速率应下降。分类问题转换为回归问题。训练步数设置为3以便测试。设置verbose为1，每步输出结果，以便了解模型训练进度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nolearn.lasagne <span class="keyword">import</span> NeuralNet</span><br><span class="line"><span class="keyword">from</span> lasagne.nonlinearities <span class="keyword">import</span> sigmoid, softmax</span><br><span class="line">nnet = NeuralNet(layers=layers,</span><br><span class="line">                 input_shape=(<span class="literal">None</span>, <span class="number">3</span>, <span class="number">32</span>, <span class="number">32</span>),</span><br><span class="line">                 conv1_num_filters=<span class="number">32</span>,</span><br><span class="line">                 conv1_filter_size=(<span class="number">3</span>, <span class="number">3</span>),</span><br><span class="line">                 conv2_num_filters=<span class="number">64</span>,</span><br><span class="line">                 conv2_filter_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                 conv3_num_filters=<span class="number">128</span>,</span><br><span class="line">                 conv3_filter_size=(<span class="number">2</span>, <span class="number">2</span>),</span><br><span class="line">                 pool1_pool_size=(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">                 pool2_pool_size=(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">                 pool3_pool_size=(<span class="number">2</span>,<span class="number">2</span>),</span><br><span class="line">                 hidden4_num_units=<span class="number">500</span>,</span><br><span class="line">                 hidden5_num_units=<span class="number">500</span>,</span><br><span class="line">                 output_num_units=<span class="number">10</span>,</span><br><span class="line">                 output_nonlinearity=softmax,</span><br><span class="line">                 update_learning_rate=<span class="number">0.01</span>,</span><br><span class="line">                 update_momentum=<span class="number">0.9</span>,</span><br><span class="line">                 regression=<span class="literal">True</span>,</span><br><span class="line">                 max_epochs=<span class="number">3</span>,</span><br><span class="line">                 verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure></li><li><p>训练神经网络，进行测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nnet.fit(X_train, y_train)</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">y_pred = nnet.predict(X_test)</span><br><span class="line">print(f1_score(y_test.argmax(axis=<span class="number">1</span>), y_pred.argmax(axis=<span class="number">1</span>), average=<span class="string">'micro'</span>))</span><br></pre></td></tr></table></figure></li></ul><h2 id="第十二章-大数据处理"><a href="#第十二章-大数据处理" class="headerlink" title="第十二章 大数据处理"></a>第十二章 大数据处理</h2><h3 id="12-1-大数据"><a href="#12-1-大数据" class="headerlink" title="12.1 大数据"></a>12.1 大数据</h3><ul><li>大数据的特点：<ul><li>海量：数据量大</li><li>高速：数据分析速度快</li><li>多样：数据集有多种形式</li><li>准确：很难确定采集到的数据是否准确。</li></ul></li><li>大数据无法加载到内存中。</li></ul><h3 id="12-2-大数据应用场景和目标"><a href="#12-2-大数据应用场景和目标" class="headerlink" title="12.2 大数据应用场景和目标"></a>12.2 大数据应用场景和目标</h3><ul><li>应用场景：<ul><li>搜索引擎</li><li>科学实验</li><li>政府数据处理</li><li>交通管理</li><li>改善客户体验，降低支出</li><li>提高公司经营管理的自动化程度，改善产品和服务质量</li><li>监测网络流量，寻找大型网络的恶意软件感染。</li></ul></li></ul><h3 id="12-3-MapReduce"><a href="#12-3-MapReduce" class="headerlink" title="12.3 MapReduce"></a>12.3 MapReduce</h3><ul><li><p>谷歌出于并行计算的需要，提出了MapReduce模型，引入了容错和可伸缩特性，可用于任意大数据集的一般性计算任务。</p></li><li><p>MapReduce主要分为映射（Map）和规约（Reduce）两步。</p></li><li><p>MapReduce范式还包括排序和合并两步。</p></li><li><p>映射这一步，接收一个函数，用这个函数处理列表中的各个元素，返回和之间列表长度相等的列表，新列表的元素为函数的返回结果。</p></li><li><p>建立sum函数与a之间的映射关系。sums是生成器，在调用前不会计算。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>], [<span class="number">3</span>,<span class="number">2</span>], [<span class="number">4</span>,<span class="number">9</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>]]</span><br><span class="line">sums = map(sum, a)</span><br><span class="line"><span class="comment"># 等效为：</span></span><br><span class="line">sums = [] </span><br><span class="line"><span class="keyword">for</span> sublist <span class="keyword">in</span> a: </span><br><span class="line">    results = sum(sublist) </span><br><span class="line">    sums.append(results)</span><br></pre></td></tr></table></figure></li><li><p>规约需要对返回结果的每一个元素应用一个函数，从初始值开始，对初始值和第一个应用指定函数，得到返回结果，然后再对所得到的结果和下一个值应用指定函数，以此类推。规约函数为<code>from functools import reduce</code>三个参数分别为函数的名字，列表和初始值。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> a + b </span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">print(reduce(add, sums, <span class="number">0</span>))</span><br><span class="line"><span class="comment"># 等价于：</span></span><br><span class="line">initial = <span class="number">0</span></span><br><span class="line">current_result = initial</span><br><span class="line"><span class="keyword">for</span> element <span class="keyword">in</span> sums:</span><br><span class="line">    current_result = add(current_result, element)</span><br></pre></td></tr></table></figure></li><li><p>为了实现分布式计算，可以在映射这一步把各个二级列表及函数说明分发到不同的计算机上。计算完成后，各计算机把结果返回主计算机。然后主计算机把结果发送给另一台计算机做规约。大大节省了存储空间。</p></li><li><p>映射函数接收一键值对，返回键值对列表。如接收文档编号文本内容键值对，返回单词词频键值对。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map_word_count</span><span class="params">(document_id, document)</span>:</span></span><br><span class="line">    counts = defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> document.split():</span><br><span class="line">        counts[word] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> counts:</span><br><span class="line">        <span class="keyword">yield</span> (word, counts[word])</span><br></pre></td></tr></table></figure></li><li><p>把每个键所有值聚集到一起。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shuffle_words</span><span class="params">(results_generators)</span>:</span></span><br><span class="line">    records = defaultdict(list)</span><br><span class="line">    <span class="keyword">for</span> results <span class="keyword">in</span> results_generators:</span><br><span class="line">        <span class="keyword">for</span> word, count <span class="keyword">in</span> results:</span><br><span class="line">            records[word].append(count)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> records:</span><br><span class="line">        <span class="keyword">yield</span> (word, records[word])</span><br></pre></td></tr></table></figure></li><li><p>规约接收一键值对，返回另一键值对。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_counts</span><span class="params">(word, list_of_counts)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (word, sum(list_of_counts))</span><br></pre></td></tr></table></figure></li><li><p>获取sklearn的20个新闻语料。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line">dataset = fetch_20newsgroups(subset=<span class="string">'train'</span>)</span><br><span class="line">documents = dataset.data[:<span class="number">50</span>]</span><br></pre></td></tr></table></figure></li><li><p>执行映射操作，得到能输出键值对（单词、词频）的生成器。执行shuffle操作，生成单词和该单词在各文档出现次数的列表两项。规约，输出单词和单词在所有文档中的词频。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">map_results = map(map_word_count, range(len(documents)), documents)</span><br><span class="line">shuffle_results = shuffle_words(map_results)</span><br><span class="line">reduce_results = [reduce_counts(word, list_of_counts) <span class="keyword">for</span> word, list_of_counts <span class="keyword">in</span> shuffle_results]</span><br></pre></td></tr></table></figure></li><li><p>Hadoop是一组包括MapReduce在内的开源工具。主要组件为Hadoop MapReduce。其他处理大数据的工具有如下几种：</p><ul><li>Hadoop分布式文件系统（HDFS）：该文件系统可以将文件保存到多台计算机上，防范硬件故障，提高带宽。</li><li>YARN：用于调度应用和管理计算机集群。</li><li>Pig：用于MapReduce的高级语言。</li><li>Hive：用于管理数据仓库和进行查询。</li><li>HBase：对谷歌分布式数据库BigTable的一种实现。</li></ul></li></ul><h3 id="12-4-应用"><a href="#12-4-应用" class="headerlink" title="12.4 应用"></a>12.4 应用</h3><ul><li><p>根据博主用词习惯判断博主性别。</p></li><li><p>MapReduce常用映射对列表中的每一篇文档运行预测模型，使用规约来调整预测结果列表，以便把结果和原文档对应起来。</p></li><li><p>测试打开并读取博客内容。设置是否在博客中的标记，找到博客开始标签<code>&lt;post&gt;</code>后，将标记设置为True。找到关闭标签<code>&lt;/post&gt;</code>后，将标记值设置为False。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">all_posts = []</span><br><span class="line"><span class="keyword">with</span> codecs.open(filename, encoding=<span class="string">'utf8'</span>) <span class="keyword">as</span> inf:</span><br><span class="line">    <span class="comment"># remove leading and trailing whitespace</span></span><br><span class="line">    post_start = <span class="literal">False</span></span><br><span class="line">    post = []</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> inf:</span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="keyword">if</span> line == <span class="string">"&lt;post&gt;"</span>:</span><br><span class="line">            post_start = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> line == <span class="string">"&lt;/post&gt;"</span>:</span><br><span class="line">            post_start = <span class="literal">False</span></span><br><span class="line">            all_posts.append(<span class="string">"\n"</span>.join(post))</span><br><span class="line">            post = []</span><br><span class="line">        <span class="keyword">elif</span> post_start:</span><br><span class="line">            post.append(line)</span><br></pre></td></tr></table></figure></li><li><p>使用映射规约任务包mrjob。提供了大部分MapReduce任务所需的标准功能，既能在没有安装Hadoop的本地计算机上进行测试，也能在Hadoop服务器上测试。</p></li><li><p>创建MRJob的子类从文件中抽取博客内容。映射函数处理每一行，从文件取一行作为输入，最后生成一篇博客的所有内容，每一行都来自同一任务所在处理的文件。获取以环境变量存储的文件名。获取文件名中的性别信息。使用yield生成器表示博主性别和博客内容，便于mrlib跟踪输出。获取所有以51开始的文件，进行测试。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</span><br><span class="line">word_search_re = re.compile(<span class="string">r"[\w']+"</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ExtractPosts</span><span class="params">(MRJob)</span>:</span></span><br><span class="line">    post_start = <span class="literal">False</span></span><br><span class="line">    post = []</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, key, line)</span>:</span></span><br><span class="line">        filename = os.environ[<span class="string">"map_input_file"</span>]</span><br><span class="line">        gender = filename.split(<span class="string">"."</span>)[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            docnum = int(filename[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            docnum = <span class="number">8</span></span><br><span class="line">        <span class="keyword">if</span> re.match(<span class="string">r"file://blogs\\51.*"</span>,filename):</span><br><span class="line">            <span class="comment"># remove leading and trailing whitespace</span></span><br><span class="line">            line = line.strip()</span><br><span class="line">            <span class="keyword">if</span> line == <span class="string">"&lt;post&gt;"</span>:</span><br><span class="line">                self.post_start = <span class="literal">True</span></span><br><span class="line">            <span class="keyword">elif</span> line == <span class="string">"&lt;/post&gt;"</span>:</span><br><span class="line">                self.post_start = <span class="literal">False</span></span><br><span class="line">                <span class="keyword">yield</span> gender, repr(<span class="string">"\n"</span>.join(self.post))</span><br><span class="line">                self.post = []</span><br><span class="line">            <span class="keyword">elif</span> self.post_start:</span><br><span class="line">                self.post.append(line)</span><br></pre></td></tr></table></figure></li><li><p>执行MapReduce任务。<code>python .\extract_posts.py blogs/51* --output-dir=51blogs/blogposts</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    ExtractPosts.run()</span><br></pre></td></tr></table></figure></li><li><p><code>from mrjob.step import MRStep</code>用MRStep管理MapReduce中的每一步操作。任务分为三步：映射、规约、再映射和规约。</p></li><li><p><code>word_search_re = re.compile(r&quot;[\w&#39;]+&quot;)</code>创建用于匹配单词的正则表达式，并对其进行编译，用来查找单词的边界。</p></li><li><p>创建新类，用于训练朴素贝叶斯分类器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NaiveBayesTrainer</span><span class="params">(MRJob)</span>:</span></span><br><span class="line">    <span class="comment"># 定义MapReduce任务的各个步骤：第一步抽取单词出现的频率，第二步比较一个单词在男女博主所写博客中出现的概率，旋转较大的作为分类结果，写入输出文件。每一步中定义映射和规约函数。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">steps</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> [</span><br><span class="line">            MRStep(mapper=self.extract_words_mapping,</span><br><span class="line">                   reducer=self.reducer_count_words),</span><br><span class="line">            MRStep(reducer=self.compare_words_reducer),</span><br><span class="line">            ]</span><br><span class="line"><span class="comment"># 接收一条博客数据，获取里面所有单词，返回1. / len(all_words)，以便后续求词频，输出博主性别。使用eval将字符串转换为列表，但不安全，建议用json。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_words_mapping</span><span class="params">(self, key, value)</span>:</span></span><br><span class="line">        tokens = value.split()</span><br><span class="line">        gender = eval(tokens[<span class="number">0</span>])</span><br><span class="line">        blog_post = eval(<span class="string">" "</span>.join(tokens[<span class="number">1</span>:]))</span><br><span class="line">        all_words = word_search_re.findall(blog_post)</span><br><span class="line">        all_words = [word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> all_words]</span><br><span class="line">        <span class="comment">#for word in all_words:</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> all_words:</span><br><span class="line">            <span class="comment">#yield "&#123;0&#125;:&#123;1&#125;".format(gender, word.lower()), 1</span></span><br><span class="line">            <span class="comment">#yield (gender, word.lower()), (1. / len(all_words))</span></span><br><span class="line">            <span class="comment"># Occurence probability</span></span><br><span class="line">            <span class="keyword">yield</span> (gender, word), <span class="number">1.</span> / len(all_words)</span><br><span class="line"><span class="comment"># 汇总每个性别使用每个单词的频率，把键改为单词。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer_count_words</span><span class="params">(self, key, counts)</span>:</span></span><br><span class="line">        s = sum(counts)</span><br><span class="line">        gender, word = key <span class="comment">#.split(":")</span></span><br><span class="line">        <span class="keyword">yield</span> word, (gender, s)</span><br><span class="line"><span class="comment"># 数据将作为一致性映射类型直接传入规约函数中，规约函数会将每个单词在所有文章中的出现频率按照性别汇集到一起，输出单词和词频字典。</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compare_words_reducer</span><span class="params">(self, word, values)</span>:</span></span><br><span class="line">        per_gender = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> values:</span><br><span class="line">            gender, s = value</span><br><span class="line">            per_gender[gender] = s</span><br><span class="line">        <span class="keyword">yield</span> word, per_gender</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ratio_mapper</span><span class="params">(self, word, value)</span>:</span></span><br><span class="line">        counts = dict(value)</span><br><span class="line">        sum_of_counts = float(np.mean(counts.values()))</span><br><span class="line">        maximum_score = max(counts.items(), key=itemgetter(<span class="number">1</span>))</span><br><span class="line">        current_ratio = maximum_score[<span class="number">1</span>] / sum_of_counts</span><br><span class="line">        <span class="keyword">yield</span> <span class="literal">None</span>, (word, sum_of_counts, value)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sorter_reducer</span><span class="params">(self, key, values)</span>:</span></span><br><span class="line">        ranked_list = sorted(values, key=itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">        n_printed = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> word, sum_of_counts, scores <span class="keyword">in</span> ranked_list:</span><br><span class="line">            <span class="keyword">if</span> n_printed &lt; <span class="number">20</span>:</span><br><span class="line">                print((n_printed + <span class="number">1</span>), word, scores)</span><br><span class="line">                n_printed += <span class="number">1</span></span><br><span class="line">            <span class="keyword">yield</span> word, dict(scores)</span><br></pre></td></tr></table></figure></li><li><p>运行代码，训练朴素贝叶斯模型。<code>python .\nb_train.py 51blogs/blogposts/ --output-dir=models/</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    NaiveBayesTrainer.run()</span><br></pre></td></tr></table></figure></li><li><p>用命令<code>cat * &gt; model.txt</code>将数据文件内容追加到model.txt中。</p></li><li><p>重新定义查找单词的正则表达式。<code>word_search_re = re.compile(r&quot;[\w&#39;]+&quot;)</code></p></li><li><p>声明从指定文件名加载模型的函数。模型是一个值为字典的字典。将模型的每一行分为两部分，用eval函数获得实际的值，它们之前是用repr函数存储的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_model</span><span class="params">(model_filename)</span>:</span></span><br><span class="line">    model = defaultdict(<span class="keyword">lambda</span>: defaultdict(float))</span><br><span class="line">    <span class="keyword">with</span> open(model_filename) <span class="keyword">as</span> inf:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> inf:</span><br><span class="line">            word, values = line.split(maxsplit=<span class="number">1</span>)</span><br><span class="line">            word = eval(word)</span><br><span class="line">            values = eval(values)</span><br><span class="line">            model[word] = values</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></li><li><p>加载实际的模型。中文系统要注意另存为utf8。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_filename = os.path.join(<span class="string">"models"</span>, <span class="string">"model.txt"</span>)</span><br><span class="line">model = load_model(model_filename)</span><br></pre></td></tr></table></figure></li><li><p>创建使用模型做预测的函数。使用log防止下溢，对于模型中不存在的词，给出默认概率1e-5。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nb_predict</span><span class="params">(model, document)</span>:</span></span><br><span class="line">    words = word_search_re.findall(document)</span><br><span class="line">    probabilities = defaultdict(<span class="keyword">lambda</span> : <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> set(words):</span><br><span class="line">        probabilities[<span class="string">"male"</span>] += np.log(model[word].get(<span class="string">"male"</span>, <span class="number">1e-5</span>))</span><br><span class="line">        probabilities[<span class="string">"female"</span>] += np.log(model[word].get(<span class="string">"female"</span>, <span class="number">1e-5</span>))</span><br><span class="line">    <span class="comment"># Now find the most likely gender</span></span><br><span class="line">    most_likely_genders = sorted(probabilities.items(), key=itemgetter(<span class="number">1</span>), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> most_likely_genders[<span class="number">0</span>][<span class="number">0</span>]</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Python数据挖掘入门与实践&quot;&gt;&lt;a href=&quot;#Python数据挖掘入门与实践&quot; class=&quot;headerlink&quot; title=&quot;Python数据挖掘入门与实践&quot;&gt;&lt;/a&gt;Python数据挖掘入门与实践&lt;/h1&gt;&lt;h2 id=&quot;第一章-开始数据挖掘之旅&quot;
      
    
    </summary>
    
    
      <category term="Python笔记" scheme="http://www.guzhipin.top/tags/Python%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
  <entry>
    <title>测试打赏</title>
    <link href="http://www.guzhipin.top/2019/04/13/%E6%B5%8B%E8%AF%95%E6%89%93%E8%B5%8F/"/>
    <id>http://www.guzhipin.top/2019/04/13/测试打赏/</id>
    <published>2019-04-12T16:18:57.000Z</published>
    <updated>2019-04-12T16:22:54.846Z</updated>
    
    <content type="html"><![CDATA[<p>习近平在党的十九届一中全会上的讲话<br>这次中央全会已经完成了选举产生新一届中央领导机构的任务。党和人民把历史重任交给我们，是对我们的高度信任和殷切期望。我们要不忘初心、牢记使命，恪尽职守，勤勉工作，以身许党许国、报党报国，为决胜全面建成小康社会、夺取新时代中国特色社会主义伟大胜利而奋斗。<br>　　这次全会选举我继续担任中央委员会总书记，我衷心感谢同志们的信任。我深感自己责任重大，决心同新一届中央领导集体一道，紧紧依靠全党同志，紧紧依靠全国各族人民，尽心尽力、夙夜在公，决不辜负全党同志的信任和期待。<br>　　党的十八大以来的5年，是党和国家发展进程中极不平凡的5年，改革开放和社会主义现代化建设取得了历史性成就。面对国际局势风云变幻、国内改革发展稳定任务十分繁重的形势，党中央坚定不移高举中国特色社会主义伟大旗帜，全面加强和改善党的领导，团结带领全党全国各族人民迎难而上、开拓进取，进行具有许多新的历史特点的伟大斗争，统筹推进“五位一体”总体布局、协调推进“四个全面”战略布局，出台一系列重大方针政策，推出一系列重大举措，推进一系列重大工作，战胜一系列重大挑战，解决了许多长期想解决而没有解决的难题，办成了许多过去想办而没有办成的大事，推动党和国家事业发生历史性变革，国家经济实力、科技实力、国防实力、综合国力、国际影响力和人民获得感显著提升，为党和国家事业进步发展奠定了更加坚实的基础。<br>　　党的十八大以来我们取得的重大成就和重要经验，凝结着十八届中央委员会、中央政治局、中央政治局常委会的智慧和心血。大家牢记党和人民重托，恪守职责，勤奋工作，开拓创新，在各自岗位上都作出了杰出的成绩。现在，张德江、俞正声、刘云山、王岐山、张高丽等许多同志离开了党中央领导岗位，党和人民将铭记他们作出的重大贡献。在这里，我代表十九届中央委员会，向他们致以衷心的感谢和崇高的敬意！<br>　　刚刚闭幕的党的十九大，是在全面建成小康社会决胜阶段、中国特色社会主义进入新时代的关键时期召开的一次十分重要的大会，是一次不忘初心、牢记使命、高举旗帜、团结奋进的大会。大会高举中国特色社会主义伟大旗帜，以马克思列宁主义、毛泽东思想、邓小平理论、“三个代表”重要思想、科学发展观、新时代中国特色社会主义思想为指导，分析了国际国内形势发展变化，回顾和总结了过去5年的工作和历史性变革，作出了中国特色社会主义进入了新时代、我国社会主要矛盾已经转化为人民日益增长的美好生活需要和不平衡不充分的发展之间的矛盾等重大政治论断，阐述了新时代中国共产党的历史使命，提出了新时代中国特色社会主义思想和基本方略，确定了决胜全面建成小康社会、开启全面建设社会主义现代化国家新征程的目标，对新时代推进中国特色社会主义伟大事业和党的建设新的伟大工程作出了全面部署。<br>　　大会通过的十八届中央委员会的报告，描绘了决胜全面建成小康社会、夺取新时代中国特色社会主义伟大胜利的宏伟蓝图，进一步指明了党和国家事业的前进方向，是全党全国各族人民智慧的结晶，是我们党团结带领全国各族人民在新时代坚持和发展中国特色社会主义的政治宣言和行动纲领。大会通过的中央纪律检查委员会工作报告，总结了十八届中央纪律检查委员会的工作，充分肯定了在党中央坚强领导下纪律检查工作取得的重大成绩，宣示了我们党深入推进党风廉政建设和反腐败斗争的坚强决心和坚定意志。大会通过的党章修正案，体现了党的十八大以来党的理论创新、实践创新、制度创新取得的成果，体现了党的十九大报告确立的重大理论观点和重大战略思想，对加强党的全面领导、推进全面从严治党提出了明确要求。<br>　　全面贯彻落实党的十九大精神，为实现党的十九大确定的目标任务而奋斗，是新一届中央领导集体的重大政治任务和工作主题。新时代意味着新起点新要求，新时代呼唤着新气象新作为。当前和今后一个时期，要重点抓好以下6个方面工作。<br>　　第一，全面把握中国特色社会主义进入新时代的新要求，不断提高党和国家事业发展水平。中国特色社会主义进入了新时代，这是我国发展新的历史方位。党的十八大以来，在新中国成立特别是改革开放以来取得重大成就的基础上，我国发展站到了新的历史起点上，中国特色社会主义事业进入了新的发展阶段。这表明，中国特色社会主义事业要从第一个百年奋斗目标迈向第二个百年奋斗目标，全面建成小康社会、加快推进社会主义现代化、实现中华民族伟大复兴既面临更为光明的前景，也需要我们付出更为艰巨的努力。在新时代的征程上，全党同志一定要适应新时代中国特色社会主义的发展要求，提高战略思维、创新思维、辩证思维、法治思维、底线思维能力，增强工作的原则性、系统性、预见性、创造性，更好把握国内外形势发展变化，更好贯彻党的理论和路线方针政策，更好贯彻党的十九大确定的大政方针、发展战略、政策措施，更好推进中国特色社会主义伟大事业和党的建设新的伟大工程，团结带领全国各族人民奋力谱写全面建成小康社会、全面建设社会主义现代化国家新篇章。<br>　　第二，全面贯彻新时代中国特色社会主义思想和基本方略，不断提高全党马克思主义理论水平。新时代中国特色社会主义思想和基本方略，不是从天上掉下来的，不是主观臆想出来的，而是党的十八大以来，在新中国成立特别是改革开放以来我们党推进理论创新和实践创新的基础上，全党全国各族人民进行艰辛理论探索的成果，是全党全国各族人民创新创造的智慧结晶。生活之树常青。一种理论的产生，源泉只能是丰富生动的现实生活，动力只能是解决社会矛盾和问题的现实要求。在新时代的征程上，全党同志一定要弘扬理论联系实际的学风，紧密联系党和国家事业发生的历史性变革，紧密联系中国特色社会主义进入新时代的新实际，紧密联系我国社会主要矛盾的重大变化，紧密联系“两个一百年”奋斗目标和各项任务，自觉运用理论指导实践，使各方面工作更符合客观规律、科学规律的要求，不断提高新时代坚持和发展中国特色社会主义的能力，把党的科学理论转化为万众一心推动实现“两个一百年”奋斗目标、实现中华民族伟大复兴中国梦的强大力量。<br>　　第三，全面完成决胜全面建成小康社会各项任务，不断提高社会主义现代化建设水平。决胜全面建成小康社会，为全面建成小康社会圆满收官，是当前和今后一个时期党和国家的首要任务。党的十九大进一步明确了我们党对如期全面建成小康社会的承诺。从时间看，3年多时间并不长，转瞬即过，时间紧迫，时间不等人。从要求看，全面建成小康社会要得到人民认可、经得起历史检验，必须做到实打实、不掺任何水分。从任务看，抓重点、补短板、强弱项还有不少难关要过，特别是要坚决打好防范化解重大风险、精准脱贫、污染防治的攻坚战。完成非凡之事，要有非凡之精神和行动。决胜就是冲锋号，就是总动员。在新时代的征程上，全党同志一定要按照党的十九大对经济建设、政治建设、文化建设、社会建设、生态文明建设等作出的部署，全面完成各项任务，确保如期全面建成小康社会，并在此基础上乘势而上，开启全面建设社会主义现代化国家新征程。<br>　　第四，全面推进各领域各方面改革，不断提高国家治理体系和治理能力现代化水平。事业发展出题目，深化改革做文章。党的十九大围绕党和国家事业发展新要求，对全面深化改革提出了新任务。全党同志必须牢记，改革开放是决定当代中国命运的关键一招，也是决定实现“两个一百年”奋斗目标、实现中华民族伟大复兴的关键一招；没有改革开放，就没有中国特色社会主义，就没有今天中国兴旺发达的大好局面。党的十八大之后，我们把全面深化改革纳入“四个全面”战略布局，蹄疾步稳推进各方面改革，取得了显著成效，有力推动了各项事业发展。同时，我们也要看到，事业发展没有止境，深化改革没有穷期；事业发展全面推进，呼唤着改革全面深化。这就是我讲的，改革只有进行时，没有结束时。新时代坚持和发展中国特色社会主义，根本动力仍然是全面深化改革。在新时代的征程上，全党同志一定要适应新时代中国特色社会主义事业发展进程，牢牢把握完善和发展中国特色社会主义制度、推进国家治理体系和治理能力现代化的总目标，统筹推进各领域各方面改革，不断推进理论创新、制度创新、科技创新、文化创新以及其他各方面创新，坚决破除一切不合时宜的思想观念和体制机制弊端，突破利益固化的藩篱，为决胜全面建成小康社会、开启全面建设社会主义现代化国家新征程提供强大动力。<br>　　第五，全面落实以人民为中心的发展思想，不断提高保障和改善民生水平。为人民谋幸福，是中国共产党人的初心。我们要时刻不忘这个初心，永远把人民对美好生活的向往作为奋斗目标。党的十九大对保障和改善民生作出了全面部署。我们要始终以实现好、维护好、发展好最广大人民根本利益为最高标准，带领人民创造美好生活，让改革发展成果更多更公平惠及全体人民，使人民获得感、幸福感、安全感更加充实、更有保障、更可持续，朝着实现全体人民共同富裕不断迈进。在新时代的征程上，全党同志一定要抓住人民最关心最直接最现实的利益问题，坚持把人民群众关心的事当作自己的大事，从人民群众关心的事情做起，多谋民生之利，多解民生之忧，在幼有所育、学有所教、劳有所得、病有所医、老有所养、住有所居、弱有所扶上不断取得新进展，不断促进社会公平正义，不断促进人的全面发展、全体人民共同富裕。<br>　　第六，全面推进党的建设新的伟大工程，不断提高全面从严治党水平。坚持党的领导，坚持党要管党、全面从严治党，是进行具有许多新的历史特点的伟大斗争、推进中国特色社会主义伟大事业、实现民族复兴伟大梦想的根本保证，也是我们党紧跟时代前进步伐、始终保持先进性和纯洁性的必然要求。我们党团结带领人民进行革命、建设、改革的实践都证明，什么时候我们党自身坚强有力，什么时候党和人民事业就能无往而不胜。党的十九大总结我们坚持党的领导、加强党的建设的新鲜经验，明确提出了新时代党的建设总要求。这个总要求不是空洞的、抽象的、说教的，而是来自加强党的建设、推进全面从严治党的现实需要，来自解决党内存在的突出矛盾和问题的现实需要，来自保持党的先进性和纯洁性、增强党的创造力凝聚力战斗力的现实需要，来自永葆党的性质和宗旨、保持党同人民群众的血肉联系的现实需要，来自坚持党的执政地位、提高党的执政能力、扩大党的执政基础的现实需要。逆水行舟用力撑，一篙松劲退千寻。我们一定要深刻认识新时代中国特色社会主义对我们党自身建设提出的新要求，着眼于我们党更好担当使命，总结运用成功经验，正视解决突出问题，一刻不停歇地推动全面从严治党向纵深发展。在新时代的征程上，全党同志一定要按照新时代党的建设总要求，坚持和加强党的全面领导，坚持党要管党、全面从严治党，拿出恒心和韧劲，继续在常和长、严和实、深和细上下功夫，管出习惯、抓出成效。只要始终做到自身硬，我们党就一定能够具有坚如磐石的意志和坚不可摧的力量，就一定能够始终保持同人民群众的血肉联系，就一定能够引领承载着中国人民伟大梦想的航船破浪前进、胜利驶向光辉的彼岸。<br>　　贯彻落实党的十九大精神，还有一个重要任务，就是认真学习贯彻党章。党的十九大通过的党章修正案，体现了党的十八大以来党的理论创新、实践创新、制度创新取得的成果，对毫不动摇坚持党的全面领导、坚定不移推进全面从严治党、坚持和完善党的建设、不断提高党的建设质量提出了许多新要求。全党同志要把学习贯彻党章作为学习贯彻党的十九大精神的重要内容，作为推进“两学一做”学习教育常态化制度化的重要举措，在全党形成自觉学习党章、模范贯彻党章、严格遵守党章、坚决维护党章的良好局面，切实把党章要求贯彻到党的工作和党的建设全过程、各方面。<br>　　同志们！团结带领全国各族人民在中国特色社会主义道路上全面建成小康社会，进而全面建成社会主义现代化强国、实现中华民族伟大复兴，是新时代中国共产党的历史使命。今天，历史的接力棒交到了我们手里。担当这份重任，我们既充满信心，又如履薄冰。充满信心，是因为我们有马克思主义的真理力量，是因为我们有党的坚强领导，是因为我们有中国特色社会主义的正确道路，是因为我们有全党全军全国各族人民的伟大团结。如履薄冰，是因为中国特色社会主义需要继续艰辛探索，是因为应对各种风险和挑战需要不断披荆斩棘，是因为抵御各种腐朽思想侵蚀需要勇于自我革命。<br>　　5年前，在党的十八届一中全会上，我曾说过，崇高信仰始终是我们党的强大精神支柱，人民群众始终是我们党的坚实执政基础；只要我们永不动摇信仰、永不脱离群众，我们就能无往而不胜。这个话，今天我再强调一遍。十九届中央委员会的全体同志，一定要忠于党、忠于祖国、忠于人民，一定要心怀忧患、勇于担当、甘于奉献，一定要谦虚谨慎、不骄不躁、艰苦奋斗，全身心投入党和人民事业。这里，我提几点希望，同大家共勉。<br>　　第一，坚定理想信念。理想信念是事业和人生的灯塔，决定我们的方向和立场，也决定我们的言论和行动。高级干部特别是中央委员会的同志们更要在时代洪流中成为坚守共产党人精神追求的中流砥柱。这些年，我们查处了那么多领导干部，他们违纪违法，最后堕入犯罪的深渊，从根本上来说是理想信念的防线崩溃了。领导干部一旦丧失了理想信念，就会把握不住自己，就会迷失方向，不仅会越过做党员的底线，而且会越过做人的底线。中央委员会的每一位同志都要把坚定理想信念作为人生的头等大事，自觉为全党作出示范和表率。要自觉学习马克思主义理论，深入观察世界发展大势，深刻体察中国特色社会主义伟大实践，不断增强中国特色社会主义道路自信、理论自信、制度自信、文化自信。要善于从外国和外国政党的兴衰成败中，从我们国家和我们党的历史中，从这些年党内正反两方面的典型中，汲取经验教训，自觉挺起共产党人的精神脊梁，用实际行动让人民群众感受到理想信念和高尚人格的强大力量。理想信念不是拿来说、拿来唱的，更不是用来装点门面的，只有见诸行动才有说服力。要知行合一、言行一致，保持对理想信念的激情和执着，牢固树立正确的世界观、权力观、事业观，用自己的实际行动为坚持和发展中国特色社会主义、为实现共产主义远大理想不懈奋斗。<br>　　第二，强化政治责任。党中央权威和集中统一领导，最关键的是政治领导。看一名党员干部特别是高级干部的素质和能力，首先看政治上是否站得稳、靠得住。站得稳、靠得住，最重要的就是要牢固树立“四个意识”，自觉在思想上政治上行动上同党中央保持高度一致，坚决维护党中央权威和集中统一领导，在各项工作中毫不动摇、百折不挠贯彻落实党中央决策部署，不打任何折扣，不耍任何小聪明，不搞任何小动作。中央委员会的每一位同志都要旗帜鲜明讲政治，自觉以马克思主义政治家的标准严格要求自己，找准政治站位，增强政治意识，强化政治担当。要注重提高政治能力，特别是把握方向、把握大势、把握全局的能力和保持政治定力、驾驭政治局面、防范政治风险的能力。谋划事业发展，制定政策措施，培养干部人才，推动工作落实，都要着眼于我们党执政地位巩固和增强，着眼于党和人民事业发展。要严格遵守政治纪律和政治规矩，全面执行党内政治生活准则，确保党中央政令畅通，确保局部服从全局，确保各项工作坚持正确政治方向。<br>　　第三，全面增强本领。当今世界正面临着前所未有的大变局，中国特色社会主义进入了新时代。党内外、国内外环境的深刻变化，工作对象和工作条件的深刻变化，知识更新周期的大大缩短，对我们的本领提出了许多新要求。所以，党的十九大特别强调，我们党既要政治过硬，也要本领高强，并就全面增强执政本领提出了具体要求。这是有很强针对性的。中央委员会的每一位同志都要认识到，位高并不意味着能力就自然提高，权重并不意味着本领就自然增强。大家要有知识不足、本领不足、能力不足的紧迫感，自觉加强学习、加强实践，永不自满，永不懈怠。我们要适应党和国家工作的新进展，努力增强各方面本领，包括学习本领、政治领导本领、改革创新本领、科学发展本领、依法执政本领、群众工作本领、狠抓落实本领、驾驭风险本领，都必须着力强化。这些年来，我一直强调要加强干部队伍专业化建设，是因为随着改革开放和社会主义现代化建设不断向前推进，各项工作对专业化、专门化、精细化提出了越来越高的要求，采取一般化、大呼隆、粗放型的领导方式和领导方法是完全不能适应的。中央委员会的每一位同志都要勤于学习、善于学习，始终保持虚怀若谷、如饥似渴的学习状态，努力打造又博又专、推陈出新的素养结构。既要向书本学又要向实践学，既要向领导和同事学又要向专家、基层和群众学，既要向传统学又要向现代学，努力成为兼收并蓄、融会贯通的通达之才。<br>　　第四，扎实改进作风。干部作风是人民群众观察评价党风的晴雨表。党的十八大以来的实践证明，作风建设必须以上率下，用钉钉子精神抓落实。抓好全党作风建设，首先要抓好中央委员会作风建设。新一届中央委员会务必保持党的十八大以来业已形成的党风建设的良好势头，并争取做得更好。中央委员会的每一位同志都要勤勤恳恳为民，兢兢业业干事，清清白白做人。勤勤恳恳为民，就是要践行全心全意为人民服务的根本宗旨，做人民公仆，始终把人民群众安危冷暖放在心上，想问题、作决策、抓工作坚持从群众中来、到群众中去，时时做到与群众同甘苦、共忧乐、共奋进。兢兢业业干事，就是要确立献身党和人民事业的崇高情怀，聚精会神履行党和人民赋予的神圣职责，实干苦干，不务虚功，夙兴夜寐，勤奋工作，以一流业绩回报党和人民的信任和重托。清清白白做人，就是要一身正气、两袖清风，自觉遵守廉洁自律准则，自觉遵守中央八项规定精神，自觉接受监督，敬畏人民、敬畏组织、敬畏法纪，公正用权、依法用权、廉洁用权，拒腐蚀、永不沾，决不搞特权，决不以权谋私，做一个堂堂正正的共产党人。我们的领导干部不仅要自身过得硬，还要管好家属和身边工作人员，履行好自己负责领域的党风廉政建设责任，坚决同各种不正之风和腐败现象作斗争。<br>　　当前，国际形势继续发生深刻复杂变化，世界力量对比有利于保持国际局势总体稳定，同时世界和平与发展面临诸多严峻挑战。我国国内形势总的很好，同时我们在前进道路上也面临一些亟待解决的突出矛盾和问题。我们一定要增强忧患意识、做到居安思危，保持战略定力，坚定必胜信念，大胆开展工作，全面做好改革发展稳定各项工作，着力破解突出矛盾和问题，有效防范和化解各种风险，努力实现全年经济社会发展预期目标，为明年工作打下坚实基础。<br>　　这里，我还要强调一个问题，就是要在全党大兴调查研究之风。我说过，调查研究是谋事之基、成事之道，没有调查就没有发言权，没有调查就没有决策权。调查研究是我们做好工作的基本功。党的十九大明确了坚持和发展新时代中国特色社会主义的大政方针，作出了一系列重大工作部署，提出了一系列重大举措，关键是抓好贯彻落实。正确的决策离不开调查研究，正确的贯彻落实同样也离不开调查研究。中央委员会的每一位同志都要积极开展调查研究，要扑下身子、沉到一线，迈开步子、走出院子，到车间码头，到田间地头，到市场社区，亲自察看、亲身体验。调查研究要紧扣人民群众生产生活，紧扣经济社会发展实际，紧扣全面从严治党面临的现实问题，紧扣贯彻落实党的十九大精神需要解决的问题。既要到工作局面好和先进的地方去总结经验，又要到困难较多、情况复杂、矛盾尖锐的地方去研究问题，特别是要多到群众意见多的地方去，多到工作做得差的地方去，既要听群众的顺耳话，也要听群众的逆耳言，这样才能听到实话、察到实情、收到实效。各级干部特别是领导干部要结合贯彻落实党的十九大精神真正动起来、深下去，切实把存在的矛盾和问题搞清搞透，把各项工作做实做好。<br>　　同志们！让我们更加紧密地团结起来，高举中国特色社会主义伟大旗帜，奋发进取、埋头苦干，勇于开拓、勇于创新，为实现党的十九大确定的目标任务而奋斗！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;习近平在党的十九届一中全会上的讲话&lt;br&gt;这次中央全会已经完成了选举产生新一届中央领导机构的任务。党和人民把历史重任交给我们，是对我们的高度信任和殷切期望。我们要不忘初心、牢记使命，恪尽职守，勤勉工作，以身许党许国、报党报国，为决胜全面建成小康社会、夺取新时代中国特色社会主
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>测试文章</title>
    <link href="http://www.guzhipin.top/2019/04/12/%E6%B5%8B%E8%AF%95%E6%96%87%E7%AB%A0/"/>
    <id>http://www.guzhipin.top/2019/04/12/测试文章/</id>
    <published>2019-04-12T14:29:15.000Z</published>
    <updated>2019-04-12T14:29:15.032Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
</feed>
